{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "import os.path\n",
    "\n",
    "# NLTK is a leading platform for building Python programs to work with human language data.\n",
    "# It provides easy-to-use interfaces to over 50 corpora and lexical resources such as WordNet,\n",
    "# along with a suite of text processing libraries for classification, tokenization, stemming,\n",
    "# tagging, parsing, and semantic reasoning, wrappers for industrial-strength NLP libraries,\n",
    "# and an active discussion forum. (source: nltk.org)\n",
    "import nltk\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "# Gensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora.\n",
    "# Target audience is the natural language processing (NLP) and information retrieval (IR) community. (source: https://pypi.org/project/gensim/)\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "from gensim.models import LsiModel\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "\n",
    "# import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\LaptopNUC\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(doc_set):\n",
    "    \"\"\"\n",
    "    Input  : document list\n",
    "    Purpose: preprocess text (tokenize, removing stopwords, and stemming)\n",
    "    Output : preprocessed text\n",
    "    \"\"\"\n",
    "    # initialize regex tokenizer\n",
    "    tokenizer = RegexpTokenizer(r'\\w+')\n",
    "    # create English stop words list\n",
    "    en_stop = set(stopwords.words('english'))\n",
    "    # Create p_stemmer of class PorterStemmer\n",
    "    p_stemmer = PorterStemmer()\n",
    "    # list for tokenized documents in loop\n",
    "    texts = []\n",
    "    # loop through document list\n",
    "    for i in doc_set:\n",
    "        # clean and tokenize document string\n",
    "        raw = i.lower()\n",
    "        tokens = tokenizer.tokenize(raw)\n",
    "        # remove stop words from tokens\n",
    "        stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "        # stem tokens\n",
    "        stemmed_tokens = [p_stemmer.stem(i) for i in stopped_tokens]\n",
    "        # add tokens to list\n",
    "        texts.append(stemmed_tokens)\n",
    "    return texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_corpus = [\"cat dog horse\", \n",
    "             \"cat horse pig\", \n",
    "             \"horse pig cow\", \n",
    "             \"baseball golf hockey\",\n",
    "            \"golf curling hockey\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = preprocess_data(my_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cat', 'dog', 'hors'],\n",
       " ['cat', 'hors', 'pig'],\n",
       " ['hors', 'pig', 'cow'],\n",
       " ['basebal', 'golf', 'hockey'],\n",
       " ['golf', 'curl', 'hockey']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_corpus(doc_clean):\n",
    "    \"\"\"\n",
    "    Input  : clean document\n",
    "    Purpose: create term dictionary of our courpus and Converting list of documents (corpus) into Term-Document Matrix\n",
    "    Output : term dictionary and Term-Document Matrix\n",
    "    \"\"\"\n",
    "    # Creating the term dictionary of our courpus, where every unique term is assigned an index.\n",
    "    dictionary = corpora.Dictionary(doc_clean)\n",
    "    # Converting list of documents (corpus) into Term-Document Matrix using dictionary prepared above.\n",
    "    term_doc_matrix = [dictionary.doc2bow(doc) for doc in doc_clean]\n",
    "    # generate LDA model\n",
    "    return dictionary,term_doc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_gensim_lsa_model(doc_clean,number_of_topics,words):\n",
    "    \"\"\"\n",
    "    Input  : clean document, number of topics and number of words associated with each topic\n",
    "    Purpose: create LSA model using gensim\n",
    "    Output : return LSA model\n",
    "    \"\"\"\n",
    "    dictionary,term_doc_matrix=prepare_corpus(doc_clean)\n",
    "    # generate LSA model\n",
    "    # Note another traditional term for the same method\n",
    "    # is \"Latent Semantic Indexing\" (LSI)\n",
    "    # gensim happens to use the term \"LSI\" instead of \"LSA\"\n",
    "    lsamodel = LsiModel(term_doc_matrix, num_topics=number_of_topics, id2word = dictionary)  # train model\n",
    "    print(lsamodel.print_topics(num_topics=number_of_topics, num_words=words))\n",
    "    return lsamodel, term_doc_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, '0.684*\"hors\" + 0.469*\"cat\" + 0.469*\"pig\" + 0.215*\"dog\" + 0.215*\"cow\" + 0.000*\"curl\" + -0.000*\"basebal\" + -0.000*\"golf\" + -0.000*\"hockey\"'), (1, '0.632*\"golf\" + 0.632*\"hockey\" + 0.316*\"basebal\" + 0.316*\"curl\" + -0.000*\"cow\" + 0.000*\"cat\" + -0.000*\"dog\" + 0.000*\"pig\" + 0.000*\"hors\"')]\n"
     ]
    }
   ],
   "source": [
    "lsa, tdm = create_gensim_lsa_model(docs, 2, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cat', 'dog', 'hors', 'pig', 'cow', 'basebal', 'golf', 'hockey', 'curl']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Terms\n",
    "[ lsa.id2word[i] for i in range(lsa.num_terms) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4.69190324e-01, -1.38830224e-15],\n",
       "       [ 2.14620373e-01,  9.03729823e-16],\n",
       "       [ 6.83810697e-01,  4.36031296e-16],\n",
       "       [ 4.69190324e-01, -4.26226021e-16],\n",
       "       [ 2.14620373e-01,  1.84626168e-15],\n",
       "       [-1.06071413e-16,  3.16227766e-01],\n",
       "       [-2.29988571e-16,  6.32455532e-01],\n",
       "       [-2.18948590e-16,  6.32455532e-01],\n",
       "       [-1.04530946e-16,  3.16227766e-01]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term representations\n",
    "U = lsa.projection.u\n",
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cat', 'dog', 'hors'],\n",
       " ['cat', 'hors', 'pig'],\n",
       " ['hors', 'pig', 'cow'],\n",
       " ['basebal', 'golf', 'hockey'],\n",
       " ['golf', 'curl', 'hockey']]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Documents\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.54177433, 0.64262054, 0.54177433, 0.        , 0.        ],\n",
       "       [0.        , 0.        , 0.        , 0.70710679, 0.70710679]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Document representations\n",
    "V = gensim.matutils.corpus2dense(lsa[tdm], len(lsa.projection.s)).T / lsa.projection.s\n",
    "V.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.54195274e-01,  3.01511338e-01,  2.54195274e-01, -9.81677937e-16,\n",
       "       -9.81677937e-16])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Similarity from \"cat\" to each document. (Dot product)\n",
    "\n",
    "(U[0,:] * V).sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cat', 'dog', 'hors'],\n",
       " ['cat', 'hors', 'pig'],\n",
       " ['hors', 'pig', 'cow'],\n",
       " ['basebal', 'golf', 'hockey'],\n",
       " ['golf', 'curl', 'hockey']]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Term 'cat' is similar to first three documents. But wait!\n",
    "\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# THIRD DOCUMENT DOESN'T EVEN CONTAIN CAT!!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
