{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grade: /100 points\n",
    "\n",
    "# Assignment 01: Supervised learning, Linear models, and Loss functions\n",
    "\n",
    "In this assignment, you're going to write your own methods to fit a linear model using OLS and LAD cost functions.  \n",
    "\n",
    "## Data set \n",
    "\n",
    "For this assignment, we will examine some data representing possums in Australia and New Guinea. The data frame contains 46 observations on the following 6 variables:\n",
    "\n",
    "* sex: Sex, either m (male) or f (female).\n",
    "* age: Age in years.\n",
    "* headL: Head length, in mm.\n",
    "* skullW: Skull width, in mm.\n",
    "* totalL: Total length, in cm.\n",
    "* tailL: Tail length, in cm.\n",
    "\n",
    "## Follow These Steps Before Submitting\n",
    "Once you are finished, ensure to complete the following steps.\n",
    "\n",
    "1.  Restart your kernel by clicking 'Kernel' > 'Restart & Run All'.\n",
    "\n",
    "2.  Fix any errors which result from this.\n",
    "\n",
    "3.  Repeat steps 1. and 2. until your notebook runs without errors.\n",
    "\n",
    "4.  Submit your completed notebook to OWL by the deadline.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before you start recall that\n",
    "\n",
    "L1 loss function (sum of magnitudes, used for LAD model):\n",
    "\n",
    "$$L_1(\\theta) = \\sum_{i=1}^{n} \\lvert {y_i-\\hat{y_i}} \\rvert$$\n",
    "\n",
    "L2 loss function (RSS, residual sum of squares, used for OLS model):\n",
    "\n",
    "$$L_2(\\theta) = \\sum_{i=1}^{n} ({y_i-\\hat{y_i}})^2$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Global Toolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.stats as ss \n",
    "import scipy.optimize as so\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: OLS Regression\n",
    "### Question 1.1:  /10 points\n",
    "\n",
    "\n",
    "Read in the `possum.csv` file as a `pandas.DataFrame`.  Investigate the relationship between the possum's age and its tail length by plotting a scatter plot of the `age` and `tailL` columns. Add an `alpha` (transparency of the plotted dots) in case some data are overlapping. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-17T18:07:06.369752800Z",
     "start_time": "2023-09-17T18:07:06.269172100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# Read in the data with pandas\n",
    "possum_data = pd.read_csv(\"possum.csv\")\n",
    "# Make the scatter plot (don't forget the axis labels)\n",
    "possum_data.plot(kind = \"scatter\", x = \"age\", y = \"tailL\", alpha = 0.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2: /5 point\n",
    "\n",
    "Recall that the linear model, we obtain predictions by computing \n",
    "\n",
    "$$ \\hat{\\mathbf{y}} = \\mathbf{X} \\hat{\\beta} $$\n",
    "\n",
    "Here, $\\mathbf{X}$ is a design matrix which includes a column of ones, $\\hat{\\beta}$ are coefficients, and $\\hat{\\mathbf{y}}$ are outcomes.  Write a function `linearModelPredict` to compute linear model predictions given data and a coefficient vector.  The function should take as it's arguments a 1d-array of coefficients `b` and the design matrix `X` as a 2d-array and return linear model predictions `yp`.\n",
    "\n",
    "Test the function by setting \n",
    "\n",
    "```\n",
    "X = np.array([[1,0],[1,-1],[1,2]])\n",
    "b = np.array([0.1,0.3])\n",
    "```\n",
    "and call your function with these values! \n",
    "\n",
    "Report $\\hat{\\mathbf{y}}$. \n",
    "What is the dimensionality of the numpy-array that you get back? \n",
    "\n",
    "Hint:  Read the documentation for `np.dot` or the `@` operator in `numpy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T18:43:15.637007Z",
     "start_time": "2023-09-20T18:43:15.624013400Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.1 -0.2  0.7]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "import numpy as np\n",
    "\n",
    "def linearModelPredict(b,X):\n",
    "    yp = np.dot(b,X.T)\n",
    "    return yp\n",
    "\n",
    "# Always important: Test the new function you have written! \n",
    "X = np.array([[1,0],[1,-1],[1,2]])\n",
    "b = np.array([0.1,0.3])\n",
    "print(linearModelPredict(b,X))\n",
    "# By the way: What happens when b is a 2d-array? \n",
    "# using b.T instead of b\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3: /15 points\n",
    "\n",
    "Write a function `linearModelLossRSS` which computes and returns the loss function for an OLS model parameterized by $\\beta$, as well as the gradient of the loss.  The function should take as its first argument a 1d-array `beta` of coefficients for the linear model, as its second argument the design matrix `X` as a 2d-array, and as its third argument a 1d-array `y` of observed outcomes.\n",
    "\n",
    "Test the function with the values \n",
    "\n",
    "```\n",
    "X = np.array([[1,0],[1,-1],[1,2]])\n",
    "b = np.array([0.1,0.3])\n",
    "y = np.array([0,0.4,2]) \n",
    "```\n",
    "\n",
    "Report the loss and the gradient. \n",
    "\n",
    "**Written answer**: To minimize the cost do you need increase or decrease the value of the parameters? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T19:40:39.197686600Z",
     "start_time": "2023-09-20T19:40:39.176544100Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(2.06, array([0.35, 0.84, 1.33]))"
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression \n",
    "import numpy as np\n",
    "\n",
    "X = np.array([[1,0],[1,-1],[1,2]])\n",
    "b = np.array([0.1,0.3])\n",
    "y = np.array([0,0.4,2]) \n",
    "\n",
    "def linearModelLossRSS(b,X,y):\n",
    "    predY = linearModelPredict(b,X)\n",
    "    residual_sum_of_squares = 0\n",
    "    delta = np.zeros(len(y))\n",
    "    for i in range(len(y)):\n",
    "        delta[i] = (predY[i]-y[i])**2\n",
    "        residual_sum_of_squares += round((predY[i]-y[i])**2, 2)\n",
    "        #print(linearModelPredict(b,X)[i], y[i])\n",
    "        #print(residual_sum_of_squares)\n",
    "    gradient = np.gradient(delta)\n",
    "    \n",
    "    return (residual_sum_of_squares, gradient)\n",
    "\n",
    "linearModelLossRSS(b,X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.4:  /15 points. \n",
    "\n",
    "Now that you've implemented a loss function in question 1.3, it is now time to minimize it!\n",
    "\n",
    "Write a function `linearModelFit` to fit a linear model.  The function should take as its first argument the design matrix `X` as a 2d-array, as its second argument a 1d-array `y` of outcomes, and as its third argument a function  `lossfcn` which returns as a tuple the value of the loss, as well as the gradient of the loss. As a result, it should return the estimated betas and the R2. \n",
    "\n",
    "**Hint**: Using `scipy.optimize.minimize` to minimize the customized loss function\n",
    "\n",
    "\n",
    "Test the function with the values: \n",
    "```\n",
    "X = np.array([[1,0],[1,-1],[1,2]])\n",
    "y = np.array([0,0.4,2]) \n",
    "```\n",
    "\n",
    "Report best parameters and the fitted R2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-09-20T19:49:35.879448800Z",
     "start_time": "2023-09-20T19:49:35.729011400Z"
    }
   },
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mIndexError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[38], line 12\u001B[0m\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m RESULT;\n\u001B[0;32m     10\u001B[0m     \u001B[38;5;66;03m#return (estimated_betas,R2)\u001B[39;00m\n\u001B[1;32m---> 12\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[43mlinearModelFit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m)\n",
      "Cell \u001B[1;32mIn[38], line 8\u001B[0m, in \u001B[0;36mlinearModelFit\u001B[1;34m(X, y, lossfcn)\u001B[0m\n\u001B[0;32m      6\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mlinearModelFit\u001B[39m(X,y,lossfcn \u001B[38;5;241m=\u001B[39m linearModelLossRSS):\n\u001B[0;32m      7\u001B[0m     bstart \u001B[38;5;241m=\u001B[39m [\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m0\u001B[39m,\u001B[38;5;241m0\u001B[39m]\n\u001B[1;32m----> 8\u001B[0m     RESULT \u001B[38;5;241m=\u001B[39m \u001B[43mso\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mminimize\u001B[49m\u001B[43m(\u001B[49m\u001B[43mlossfcn\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbstart\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mT\u001B[49m\u001B[43m,\u001B[49m\u001B[43my\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjac\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\n\u001B[0;32m      9\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m RESULT\n",
      "File \u001B[1;32m~\\OneDrive - The University of Western Ontario\\PrevCourses\\5th Year\\A\\DATASCI 3000A\\Assignments\\1\\venv\\Lib\\site-packages\\scipy\\optimize\\_minimize.py:705\u001B[0m, in \u001B[0;36mminimize\u001B[1;34m(fun, x0, args, method, jac, hess, hessp, bounds, constraints, tol, callback, options)\u001B[0m\n\u001B[0;32m    703\u001B[0m     res \u001B[38;5;241m=\u001B[39m _minimize_cg(fun, x0, args, jac, callback, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n\u001B[0;32m    704\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m meth \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbfgs\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m--> 705\u001B[0m     res \u001B[38;5;241m=\u001B[39m \u001B[43m_minimize_bfgs\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfun\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjac\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mcallback\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43moptions\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    706\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m meth \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnewton-cg\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[0;32m    707\u001B[0m     res \u001B[38;5;241m=\u001B[39m _minimize_newtoncg(fun, x0, args, jac, hess, hessp, callback,\n\u001B[0;32m    708\u001B[0m                              \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39moptions)\n",
      "File \u001B[1;32m~\\OneDrive - The University of Western Ontario\\PrevCourses\\5th Year\\A\\DATASCI 3000A\\Assignments\\1\\venv\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:1419\u001B[0m, in \u001B[0;36m_minimize_bfgs\u001B[1;34m(fun, x0, args, jac, callback, gtol, norm, eps, maxiter, disp, return_all, finite_diff_rel_step, xrtol, **unknown_options)\u001B[0m\n\u001B[0;32m   1416\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m maxiter \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m   1417\u001B[0m     maxiter \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mlen\u001B[39m(x0) \u001B[38;5;241m*\u001B[39m \u001B[38;5;241m200\u001B[39m\n\u001B[1;32m-> 1419\u001B[0m sf \u001B[38;5;241m=\u001B[39m \u001B[43m_prepare_scalar_function\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfun\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mjac\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepsilon\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43meps\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m   1420\u001B[0m \u001B[43m                              \u001B[49m\u001B[43mfinite_diff_rel_step\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfinite_diff_rel_step\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m   1422\u001B[0m f \u001B[38;5;241m=\u001B[39m sf\u001B[38;5;241m.\u001B[39mfun\n\u001B[0;32m   1423\u001B[0m myfprime \u001B[38;5;241m=\u001B[39m sf\u001B[38;5;241m.\u001B[39mgrad\n",
      "File \u001B[1;32m~\\OneDrive - The University of Western Ontario\\PrevCourses\\5th Year\\A\\DATASCI 3000A\\Assignments\\1\\venv\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:383\u001B[0m, in \u001B[0;36m_prepare_scalar_function\u001B[1;34m(fun, x0, jac, args, bounds, epsilon, finite_diff_rel_step, hess)\u001B[0m\n\u001B[0;32m    379\u001B[0m     bounds \u001B[38;5;241m=\u001B[39m (\u001B[38;5;241m-\u001B[39mnp\u001B[38;5;241m.\u001B[39minf, np\u001B[38;5;241m.\u001B[39minf)\n\u001B[0;32m    381\u001B[0m \u001B[38;5;66;03m# ScalarFunction caches. Reuse of fun(x) during grad\u001B[39;00m\n\u001B[0;32m    382\u001B[0m \u001B[38;5;66;03m# calculation reduces overall function evaluations.\u001B[39;00m\n\u001B[1;32m--> 383\u001B[0m sf \u001B[38;5;241m=\u001B[39m \u001B[43mScalarFunction\u001B[49m\u001B[43m(\u001B[49m\u001B[43mfun\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mx0\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mgrad\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mhess\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    384\u001B[0m \u001B[43m                    \u001B[49m\u001B[43mfinite_diff_rel_step\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mbounds\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mepsilon\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mepsilon\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    386\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m sf\n",
      "File \u001B[1;32m~\\OneDrive - The University of Western Ontario\\PrevCourses\\5th Year\\A\\DATASCI 3000A\\Assignments\\1\\venv\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:158\u001B[0m, in \u001B[0;36mScalarFunction.__init__\u001B[1;34m(self, fun, x0, args, grad, hess, finite_diff_rel_step, finite_diff_bounds, epsilon)\u001B[0m\n\u001B[0;32m    155\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf \u001B[38;5;241m=\u001B[39m fun_wrapped(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx)\n\u001B[0;32m    157\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_update_fun_impl \u001B[38;5;241m=\u001B[39m update_fun\n\u001B[1;32m--> 158\u001B[0m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_update_fun\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    160\u001B[0m \u001B[38;5;66;03m# Gradient evaluation\u001B[39;00m\n\u001B[0;32m    161\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mcallable\u001B[39m(grad):\n",
      "File \u001B[1;32m~\\OneDrive - The University of Western Ontario\\PrevCourses\\5th Year\\A\\DATASCI 3000A\\Assignments\\1\\venv\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:251\u001B[0m, in \u001B[0;36mScalarFunction._update_fun\u001B[1;34m(self)\u001B[0m\n\u001B[0;32m    249\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_update_fun\u001B[39m(\u001B[38;5;28mself\u001B[39m):\n\u001B[0;32m    250\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf_updated:\n\u001B[1;32m--> 251\u001B[0m         \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_update_fun_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    252\u001B[0m         \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf_updated \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mTrue\u001B[39;00m\n",
      "File \u001B[1;32m~\\OneDrive - The University of Western Ontario\\PrevCourses\\5th Year\\A\\DATASCI 3000A\\Assignments\\1\\venv\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:155\u001B[0m, in \u001B[0;36mScalarFunction.__init__.<locals>.update_fun\u001B[1;34m()\u001B[0m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mupdate_fun\u001B[39m():\n\u001B[1;32m--> 155\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mf \u001B[38;5;241m=\u001B[39m \u001B[43mfun_wrapped\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\OneDrive - The University of Western Ontario\\PrevCourses\\5th Year\\A\\DATASCI 3000A\\Assignments\\1\\venv\\Lib\\site-packages\\scipy\\optimize\\_differentiable_functions.py:137\u001B[0m, in \u001B[0;36mScalarFunction.__init__.<locals>.fun_wrapped\u001B[1;34m(x)\u001B[0m\n\u001B[0;32m    133\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mnfev \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;241m1\u001B[39m\n\u001B[0;32m    134\u001B[0m \u001B[38;5;66;03m# Send a copy because the user may overwrite it.\u001B[39;00m\n\u001B[0;32m    135\u001B[0m \u001B[38;5;66;03m# Overwriting results in undefined behaviour because\u001B[39;00m\n\u001B[0;32m    136\u001B[0m \u001B[38;5;66;03m# fun(self.x) will change self.x, with the two no longer linked.\u001B[39;00m\n\u001B[1;32m--> 137\u001B[0m fx \u001B[38;5;241m=\u001B[39m \u001B[43mfun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mnp\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mcopy\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    138\u001B[0m \u001B[38;5;66;03m# Make sure the function returns a true scalar\u001B[39;00m\n\u001B[0;32m    139\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m np\u001B[38;5;241m.\u001B[39misscalar(fx):\n",
      "File \u001B[1;32m~\\OneDrive - The University of Western Ontario\\PrevCourses\\5th Year\\A\\DATASCI 3000A\\Assignments\\1\\venv\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:77\u001B[0m, in \u001B[0;36mMemoizeJac.__call__\u001B[1;34m(self, x, *args)\u001B[0m\n\u001B[0;32m     75\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m__call__\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, \u001B[38;5;241m*\u001B[39margs):\n\u001B[0;32m     76\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\" returns the function value \"\"\"\u001B[39;00m\n\u001B[1;32m---> 77\u001B[0m     \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_compute_if_needed\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     78\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_value\n",
      "File \u001B[1;32m~\\OneDrive - The University of Western Ontario\\PrevCourses\\5th Year\\A\\DATASCI 3000A\\Assignments\\1\\venv\\Lib\\site-packages\\scipy\\optimize\\_optimize.py:71\u001B[0m, in \u001B[0;36mMemoizeJac._compute_if_needed\u001B[1;34m(self, x, *args)\u001B[0m\n\u001B[0;32m     69\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m np\u001B[38;5;241m.\u001B[39mall(x \u001B[38;5;241m==\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx) \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjac \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m     70\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mx \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39masarray(x)\u001B[38;5;241m.\u001B[39mcopy()\n\u001B[1;32m---> 71\u001B[0m     fg \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfun\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m     72\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mjac \u001B[38;5;241m=\u001B[39m fg[\u001B[38;5;241m1\u001B[39m]\n\u001B[0;32m     73\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_value \u001B[38;5;241m=\u001B[39m fg[\u001B[38;5;241m0\u001B[39m]\n",
      "Cell \u001B[1;32mIn[28], line 13\u001B[0m, in \u001B[0;36mlinearModelLossRSS\u001B[1;34m(b, X, y)\u001B[0m\n\u001B[0;32m     11\u001B[0m delta \u001B[38;5;241m=\u001B[39m np\u001B[38;5;241m.\u001B[39mzeros(\u001B[38;5;28mlen\u001B[39m(y))\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m i \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(\u001B[38;5;28mlen\u001B[39m(y)):\n\u001B[1;32m---> 13\u001B[0m     delta[i] \u001B[38;5;241m=\u001B[39m (\u001B[43mpredY\u001B[49m\u001B[43m[\u001B[49m\u001B[43mi\u001B[49m\u001B[43m]\u001B[49m\u001B[38;5;241m-\u001B[39my[i])\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m\n\u001B[0;32m     14\u001B[0m     residual_sum_of_squares \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m \u001B[38;5;28mround\u001B[39m((predY[i]\u001B[38;5;241m-\u001B[39my[i])\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m, \u001B[38;5;241m2\u001B[39m)\n\u001B[0;32m     15\u001B[0m     \u001B[38;5;66;03m#print(linearModelPredict(b,X)[i], y[i])\u001B[39;00m\n\u001B[0;32m     16\u001B[0m     \u001B[38;5;66;03m#print(residual_sum_of_squares)\u001B[39;00m\n",
      "\u001B[1;31mIndexError\u001B[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "X = np.array([[1,0],[1,-1],[1,2]])\n",
    "y = np.array([0,0.4,2]) \n",
    "\n",
    "import scipy.optimize as so\n",
    "\n",
    "def linearModelFit(X,y,lossfcn = linearModelLossRSS):\n",
    "    bstart = [0,0,0]\n",
    "    RESULT = so.minimize(lossfcn, bstart, args = (X.T,y), jac = True)\n",
    "    return RESULT;\n",
    "    #return (estimated_betas,R2)\n",
    "\n",
    "print(linearModelFit(X,y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.5: /15 points\n",
    "\n",
    "Use the above functions to fit your model to the possum data. Then use your model and the fitted parameters to make predictions along a grid of equally spaced possum ages.  \n",
    "\n",
    "**Hint** : Don't forget to include a column of ones in your design matrix to allow bias\n",
    "\n",
    "Plot the data and add a line for the predicted values. You can get these by generating a new X-matrix with equally space ages (using for example np.linspace). Also report the R2 value for the fit. You can do this by either printing out the R2 of the fit or putting it on your plot via the `annotate` function in matplotlib.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the design matrix using np.c_ \n",
    "# y = ...\n",
    "# X =  np.c_[...]\n",
    "# Call your fitting function \n",
    "\n",
    "# Create the scatter plot (see question 1.1)\n",
    "\n",
    "# Create a new X matrix with equally space data \n",
    "\n",
    "# Add the line to the graph \n",
    "\n",
    "# Report R2 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: LAD Regression\n",
    "\n",
    "### Question 2.1:  /13 points\n",
    "\n",
    "In the previous section, we worked with the squared loss.  Now, we'll implement a linear model with least absolute deviation loss.\n",
    "\n",
    "Write a function `linearModelLossLAD` which computes the least absolute deviation loss function for a linear model  parameterized by $\\beta$, as well as the gradient of the loss.  The function should take as its first argument a 1d-array `beta` of coefficients for the linear model, as its second argument the design matrix `X` as a 2d-array, and as its third argument a 1d-array `y` of observed outcomes.\n",
    "\n",
    "Test the function with the values \n",
    "\n",
    "```\n",
    "X = np.array([[1,0],[1,-1],[1,2]])\n",
    "b = np.array([0.1,0.3])\n",
    "y = np.array([0,0.4,2]) \n",
    "```\n",
    "\n",
    "Report the loss and the gradient. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linearModelLossLAD(b,X,y):\n",
    "\n",
    "    return (sum_abs_dev,grad)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2: /8 points\n",
    "\n",
    "\n",
    "Use the above functions to fit your LAD model. Use your model to make predictions along a grid of equally spaced possum ages.  Once fit, add the fitted line to the scatter plot as in question 1.5.  Also report the R2-value. \n",
    "\n",
    "**Written answer**: What is the difference in the fit obtained with an L1 as compared to the L2 cost function? Which one has a higher R2 value? Why?  \n",
    "\n",
    "Note: If you recieve an error from the optimizer, it may be because the loss function for the LAD model is not differentiable at its minimum.  This will lead to some gradient based optimizers to fail to converge.  If this happens to you then pass `method=\"Powell\"` to `scipy.optimize.minimize`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Written answer: The LAD fit does not give as much weight to the outlier (9,55) as the OLS fit. The R2 value is lower, however. This is because OLS minimized the RSS, and therefore maximizes R2.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: OLS Regression using Sklearn\n",
    "### Question 3.1: /7 points\n",
    "\n",
    "Fit an OLS model to the possum data with the `linear_model` module from the `sklearn` package by using the `LinearRegression` class.  In no more than two sentences, comment on the rsquared values from `sklearn` and the rsquared values from your models. Are they similar?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: A new loss function\n",
    "### Question 4.1: /4 points\n",
    "\n",
    "Assume that the following function has been proposed to you to use as a new loss function for regression:\n",
    "\n",
    "$$L_3(\\hat{\\beta}) = \\sum_{i=1}^{n} ({y_i-\\hat{y_i}})^3$$\n",
    "\n",
    "What do you think about this new loss function, would you accept it? Please, explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Answer in this cell **\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 4.2: /8 points\n",
    "\n",
    "Regardless of your answer to Question 4.1, taking $\\: \\hat{\\mathbf{y}} = \\mathbf{X^2} \\hat{\\beta}\\:$ as your functional form for regression, use your knowledge of calculus to calculate the gradient of $\\: L_3(\\hat{\\beta}).$ Type your answer (including the intermediate steps) in the cell below in Latex format. Note that $\\hat{\\beta}=[\\beta_0, \\beta_1]^T$, and $\\mathbf{X}$ is an $n$-by-$2$ matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "${your}_{answers}$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "c1bb765f2992f8c3473d947572f750144adb783c35189567846adef00c3476ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
