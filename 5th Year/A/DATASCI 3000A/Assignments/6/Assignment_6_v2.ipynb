{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5a4bd37",
   "metadata": {},
   "source": [
    "# Assignment 6: Feature selection and regularization\n",
    "\n",
    "# Total: /100\n",
    "\n",
    "## Instructions\n",
    "\n",
    "* Complete the assignment\n",
    "\n",
    "* Once the notebook is complete, **restart** your kernel and **rerun** your cells\n",
    "\n",
    "* Submit your completed notebook to owl by the deadline\n",
    "\n",
    "* You may use any python library functions you wish to complete the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2d5c58f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T19:58:25.098925200Z",
     "start_time": "2023-10-25T19:58:24.872659500Z"
    }
   },
   "outputs": [],
   "source": [
    "# You may need these\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn as sk\n",
    "import sklearn.linear_model as skl\n",
    "from sklearn import preprocessing\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# To get geo info of IP addresses:\n",
    "#!pip install maxminddb-geolite2\n",
    "from geolite2 import geolite2\n",
    "\n",
    "seed = 2023\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824fc0a9",
   "metadata": {},
   "source": [
    "## Question 1: /20 pts\n",
    "\n",
    "The dataset `customer_data.csv` lists certain attributes providing valuable insights into customer behavior and demographics:\n",
    "\n",
    "- **full.name**: Customer's full name\n",
    "- **ip.address**: Customer's IP address\n",
    "- **region**: Customer's geographical region\n",
    "- **age**: Customer's age\n",
    "- **items**: Number of items purchased by the customer\n",
    "- **amount**: The total amount spent by the customer\n",
    "\n",
    "Businesses can leverage this dataset to make data-driven decisions, understand customer preferences, and tailor their strategies to meet customer needs and interests.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d89159c2",
   "metadata": {},
   "source": [
    "### 1.1 Load the dataset and display the first 5 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8d464a7c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T19:58:30.687946100Z",
     "start_time": "2023-10-25T19:58:30.513407200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        full.name      ip.address  region  in.store  age  items   amount\n0   Carter Stokes             NaN       2         0   37      4   281.03\n1     Jacob Jerde             NaN       2         0   35      2   219.51\n2    Tressa Ratke  192.90.208.202       4         1   45      3  1525.70\n3  Rudolf Abshire  251.55.128.164       3         1   46      3   715.25\n4   Theresa Davis  182.19.192.186       1         1   33      4  1937.50",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>full.name</th>\n      <th>ip.address</th>\n      <th>region</th>\n      <th>in.store</th>\n      <th>age</th>\n      <th>items</th>\n      <th>amount</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Carter Stokes</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>0</td>\n      <td>37</td>\n      <td>4</td>\n      <td>281.03</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Jacob Jerde</td>\n      <td>NaN</td>\n      <td>2</td>\n      <td>0</td>\n      <td>35</td>\n      <td>2</td>\n      <td>219.51</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Tressa Ratke</td>\n      <td>192.90.208.202</td>\n      <td>4</td>\n      <td>1</td>\n      <td>45</td>\n      <td>3</td>\n      <td>1525.70</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Rudolf Abshire</td>\n      <td>251.55.128.164</td>\n      <td>3</td>\n      <td>1</td>\n      <td>46</td>\n      <td>3</td>\n      <td>715.25</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Theresa Davis</td>\n      <td>182.19.192.186</td>\n      <td>1</td>\n      <td>1</td>\n      <td>33</td>\n      <td>4</td>\n      <td>1937.50</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#your code here\n",
    "customer_data = pd.read_csv('customer_data.csv')\n",
    "display(customer_data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "230d3aac",
   "metadata": {},
   "source": [
    "### 1.2 First, remove any rows where the entry of \"Age\" column is below 18 or above 80, and then extract two new features from `ip.address`: one called `latitude` and the other `longitude`. Use the package `geolite2` for the conversion of the IP addresses to latitude and longitude. Use [pandas.DataFrame.apply](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.apply.html) to do the conversion (in a vectorized way) in one go for each new feature. Avoid using `for` loops. At the end, drop the column `ip.address` as well as any rows with a missing value. Display the first 5 rows of the new dataframe and report its shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cc0b8076",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T20:00:25.718803200Z",
     "start_time": "2023-10-25T20:00:11.119927200Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "(34303, 8)"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code here\n",
    "customer_data = customer_data[(customer_data['age'] >= 18) & (customer_data['age'] <= 80)]\n",
    "def get_lat_lon(ip):\n",
    "    try:\n",
    "        location = geolite2.reader().get(ip)\n",
    "        if location and 'location' in location:\n",
    "            return pd.Series([location['location']['latitude'], location['location']['longitude']])\n",
    "        else:\n",
    "            return pd.Series([None, None])\n",
    "    except Exception:\n",
    "        return pd.Series([None, None])\n",
    "\n",
    "\n",
    "customer_data[['latitude', 'longitude']] = customer_data['ip.address'].apply(get_lat_lon)\n",
    "df = customer_data.drop(columns=['ip.address'])\n",
    "df = df.dropna()\n",
    "\n",
    "df.head()\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08a7f1d",
   "metadata": {},
   "source": [
    "### 1.3 Perform one-hot encoding on the `region` column using pd.get_dummies(). Display the first 5 rows of the encoded dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7cae6d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T20:00:34.604224100Z",
     "start_time": "2023-10-25T20:00:34.574566500Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "        full.name      ip.address  in.store  age  items   amount  latitude  \\\n0   Carter Stokes             NaN         0   37      4   281.03       NaN   \n1     Jacob Jerde             NaN         0   35      2   219.51       NaN   \n2    Tressa Ratke  192.90.208.202         1   45      3  1525.70   42.5879   \n3  Rudolf Abshire  251.55.128.164         1   46      3   715.25       NaN   \n4   Theresa Davis  182.19.192.186         1   33      4  1937.50    1.2931   \n\n   longitude  region_2  region_3  region_4  \n0        NaN      True     False     False  \n1        NaN      True     False     False  \n2   -71.3498     False     False      True  \n3        NaN     False      True     False  \n4   103.8558     False     False     False  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>full.name</th>\n      <th>ip.address</th>\n      <th>in.store</th>\n      <th>age</th>\n      <th>items</th>\n      <th>amount</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>region_2</th>\n      <th>region_3</th>\n      <th>region_4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Carter Stokes</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>37</td>\n      <td>4</td>\n      <td>281.03</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Jacob Jerde</td>\n      <td>NaN</td>\n      <td>0</td>\n      <td>35</td>\n      <td>2</td>\n      <td>219.51</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Tressa Ratke</td>\n      <td>192.90.208.202</td>\n      <td>1</td>\n      <td>45</td>\n      <td>3</td>\n      <td>1525.70</td>\n      <td>42.5879</td>\n      <td>-71.3498</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Rudolf Abshire</td>\n      <td>251.55.128.164</td>\n      <td>1</td>\n      <td>46</td>\n      <td>3</td>\n      <td>715.25</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Theresa Davis</td>\n      <td>182.19.192.186</td>\n      <td>1</td>\n      <td>33</td>\n      <td>4</td>\n      <td>1937.50</td>\n      <td>1.2931</td>\n      <td>103.8558</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#your code here\n",
    "customer_data_encoded = pd.get_dummies(customer_data, columns=['region'], drop_first=True)\n",
    "display(customer_data_encoded.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e161881e",
   "metadata": {},
   "source": [
    "### 1.4 Calculate the natural logarithm of the column reporting clients' total amount spent and store it as a new column `log_amount`. Create your design matrix `X` and target vector `y` with `log_amount` as target (No training/test splitting yet).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a06992d0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T20:02:35.598494100Z",
     "start_time": "2023-10-25T20:02:35.584564Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "       in.store  age  items  latitude  longitude  region_2  region_3  region_4\n0             0   37      4       NaN        NaN      True     False     False\n1             0   35      2       NaN        NaN      True     False     False\n2             1   45      3   42.5879   -71.3498     False     False      True\n3             1   46      3       NaN        NaN     False      True     False\n4             1   33      4    1.2931   103.8558     False     False     False\n...         ...  ...    ...       ...        ...       ...       ...       ...\n79995         1   71      3   37.3042  -122.0946     False     False     False\n79996         0   59      7       NaN        NaN     False      True     False\n79997         0   54      1       NaN        NaN      True     False     False\n79998         1   49      4   45.3548   -75.5773     False     False     False\n79999         1   30      1   56.4977    84.9744     False      True     False\n\n[78306 rows x 8 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>in.store</th>\n      <th>age</th>\n      <th>items</th>\n      <th>latitude</th>\n      <th>longitude</th>\n      <th>region_2</th>\n      <th>region_3</th>\n      <th>region_4</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>0</td>\n      <td>37</td>\n      <td>4</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>35</td>\n      <td>2</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>45</td>\n      <td>3</td>\n      <td>42.5879</td>\n      <td>-71.3498</td>\n      <td>False</td>\n      <td>False</td>\n      <td>True</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>1</td>\n      <td>46</td>\n      <td>3</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>1</td>\n      <td>33</td>\n      <td>4</td>\n      <td>1.2931</td>\n      <td>103.8558</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>79995</th>\n      <td>1</td>\n      <td>71</td>\n      <td>3</td>\n      <td>37.3042</td>\n      <td>-122.0946</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>79996</th>\n      <td>0</td>\n      <td>59</td>\n      <td>7</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>79997</th>\n      <td>0</td>\n      <td>54</td>\n      <td>1</td>\n      <td>NaN</td>\n      <td>NaN</td>\n      <td>True</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>79998</th>\n      <td>1</td>\n      <td>49</td>\n      <td>4</td>\n      <td>45.3548</td>\n      <td>-75.5773</td>\n      <td>False</td>\n      <td>False</td>\n      <td>False</td>\n    </tr>\n    <tr>\n      <th>79999</th>\n      <td>1</td>\n      <td>30</td>\n      <td>1</td>\n      <td>56.4977</td>\n      <td>84.9744</td>\n      <td>False</td>\n      <td>True</td>\n      <td>False</td>\n    </tr>\n  </tbody>\n</table>\n<p>78306 rows × 8 columns</p>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#your code here\n",
    "customer_data_encoded['log_amount'] = np.log(customer_data_encoded['amount'])\n",
    "X = customer_data_encoded.drop(columns=['full.name', 'amount', 'log_amount', 'ip.address'])\n",
    "y = customer_data_encoded['log_amount']\n",
    "X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ba9bf0",
   "metadata": {},
   "source": [
    "### 1.5 Build a new design matrix by applying polynomial expansion using `PolynomialFeatures()` on `X` with degree=2. Do not include the column with power 0 (*i.e.*, the column with all elements being 1) and make sure to not set the argument `interaction_only` to `True`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc1b1e18",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-25T20:02:12.312512200Z",
     "start_time": "2023-10-25T20:02:12.223151800Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nPolynomialFeatures does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[12], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m#your code here\u001B[39;00m\n\u001B[0;32m      2\u001B[0m poly \u001B[38;5;241m=\u001B[39m PolynomialFeatures(degree\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m2\u001B[39m, include_bias\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m)\n\u001B[1;32m----> 3\u001B[0m X_poly \u001B[38;5;241m=\u001B[39m \u001B[43mpoly\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit_transform\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\_set_output.py:157\u001B[0m, in \u001B[0;36m_wrap_method_output.<locals>.wrapped\u001B[1;34m(self, X, *args, **kwargs)\u001B[0m\n\u001B[0;32m    155\u001B[0m \u001B[38;5;129m@wraps\u001B[39m(f)\n\u001B[0;32m    156\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mwrapped\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs):\n\u001B[1;32m--> 157\u001B[0m     data_to_wrap \u001B[38;5;241m=\u001B[39m \u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    158\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data_to_wrap, \u001B[38;5;28mtuple\u001B[39m):\n\u001B[0;32m    159\u001B[0m         \u001B[38;5;66;03m# only wrap the first output for cross decomposition\u001B[39;00m\n\u001B[0;32m    160\u001B[0m         return_tuple \u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    161\u001B[0m             _wrap_data_with_container(method, data_to_wrap[\u001B[38;5;241m0\u001B[39m], X, \u001B[38;5;28mself\u001B[39m),\n\u001B[0;32m    162\u001B[0m             \u001B[38;5;241m*\u001B[39mdata_to_wrap[\u001B[38;5;241m1\u001B[39m:],\n\u001B[0;32m    163\u001B[0m         )\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:916\u001B[0m, in \u001B[0;36mTransformerMixin.fit_transform\u001B[1;34m(self, X, y, **fit_params)\u001B[0m\n\u001B[0;32m    912\u001B[0m \u001B[38;5;66;03m# non-optimized default implementation; override when a better\u001B[39;00m\n\u001B[0;32m    913\u001B[0m \u001B[38;5;66;03m# method is possible for a given clustering algorithm\u001B[39;00m\n\u001B[0;32m    914\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[0;32m    915\u001B[0m     \u001B[38;5;66;03m# fit method of arity 1 (unsupervised transformation)\u001B[39;00m\n\u001B[1;32m--> 916\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfit\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mfit_params\u001B[49m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mtransform(X)\n\u001B[0;32m    917\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m    918\u001B[0m     \u001B[38;5;66;03m# fit method of arity 2 (supervised transformation)\u001B[39;00m\n\u001B[0;32m    919\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mfit(X, y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mfit_params)\u001B[38;5;241m.\u001B[39mtransform(X)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:1152\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1145\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1147\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1148\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1149\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1150\u001B[0m     )\n\u001B[0;32m   1151\u001B[0m ):\n\u001B[1;32m-> 1152\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfit_method\u001B[49m\u001B[43m(\u001B[49m\u001B[43mestimator\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\preprocessing\\_polynomial.py:322\u001B[0m, in \u001B[0;36mPolynomialFeatures.fit\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    304\u001B[0m \u001B[38;5;129m@_fit_context\u001B[39m(prefer_skip_nested_validation\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m)\n\u001B[0;32m    305\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mfit\u001B[39m(\u001B[38;5;28mself\u001B[39m, X, y\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m):\n\u001B[0;32m    306\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m    307\u001B[0m \u001B[38;5;124;03m    Compute number of output features.\u001B[39;00m\n\u001B[0;32m    308\u001B[0m \n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    320\u001B[0m \u001B[38;5;124;03m        Fitted transformer.\u001B[39;00m\n\u001B[0;32m    321\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 322\u001B[0m     _, n_features \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_validate_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43maccept_sparse\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m\u001B[38;5;241m.\u001B[39mshape\n\u001B[0;32m    324\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdegree, Integral):\n\u001B[0;32m    325\u001B[0m         \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mdegree \u001B[38;5;241m==\u001B[39m \u001B[38;5;241m0\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39minclude_bias:\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\base.py:605\u001B[0m, in \u001B[0;36mBaseEstimator._validate_data\u001B[1;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001B[0m\n\u001B[0;32m    603\u001B[0m         out \u001B[38;5;241m=\u001B[39m X, y\n\u001B[0;32m    604\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m no_val_y:\n\u001B[1;32m--> 605\u001B[0m     out \u001B[38;5;241m=\u001B[39m \u001B[43mcheck_array\u001B[49m\u001B[43m(\u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mX\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mcheck_params\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    606\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m no_val_X \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m no_val_y:\n\u001B[0;32m    607\u001B[0m     out \u001B[38;5;241m=\u001B[39m _check_y(y, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mcheck_params)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:957\u001B[0m, in \u001B[0;36mcheck_array\u001B[1;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001B[0m\n\u001B[0;32m    951\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m    952\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFound array with dim \u001B[39m\u001B[38;5;132;01m%d\u001B[39;00m\u001B[38;5;124m. \u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m expected <= 2.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    953\u001B[0m             \u001B[38;5;241m%\u001B[39m (array\u001B[38;5;241m.\u001B[39mndim, estimator_name)\n\u001B[0;32m    954\u001B[0m         )\n\u001B[0;32m    956\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m force_all_finite:\n\u001B[1;32m--> 957\u001B[0m         \u001B[43m_assert_all_finite\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    958\u001B[0m \u001B[43m            \u001B[49m\u001B[43marray\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    959\u001B[0m \u001B[43m            \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    960\u001B[0m \u001B[43m            \u001B[49m\u001B[43mestimator_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    961\u001B[0m \u001B[43m            \u001B[49m\u001B[43mallow_nan\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mforce_all_finite\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m==\u001B[39;49m\u001B[43m \u001B[49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[38;5;124;43mallow-nan\u001B[39;49m\u001B[38;5;124;43m\"\u001B[39;49m\u001B[43m,\u001B[49m\n\u001B[0;32m    962\u001B[0m \u001B[43m        \u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m    964\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m ensure_min_samples \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m    965\u001B[0m     n_samples \u001B[38;5;241m=\u001B[39m _num_samples(array)\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:122\u001B[0m, in \u001B[0;36m_assert_all_finite\u001B[1;34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001B[0m\n\u001B[0;32m    119\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m first_pass_isfinite:\n\u001B[0;32m    120\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m\n\u001B[1;32m--> 122\u001B[0m \u001B[43m_assert_all_finite_element_wise\u001B[49m\u001B[43m(\u001B[49m\n\u001B[0;32m    123\u001B[0m \u001B[43m    \u001B[49m\u001B[43mX\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    124\u001B[0m \u001B[43m    \u001B[49m\u001B[43mxp\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mxp\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    125\u001B[0m \u001B[43m    \u001B[49m\u001B[43mallow_nan\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mallow_nan\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    126\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmsg_dtype\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mmsg_dtype\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    127\u001B[0m \u001B[43m    \u001B[49m\u001B[43mestimator_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mestimator_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    128\u001B[0m \u001B[43m    \u001B[49m\u001B[43minput_name\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43minput_name\u001B[49m\u001B[43m,\u001B[49m\n\u001B[0;32m    129\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\sklearn\\utils\\validation.py:171\u001B[0m, in \u001B[0;36m_assert_all_finite_element_wise\u001B[1;34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001B[0m\n\u001B[0;32m    154\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m estimator_name \u001B[38;5;129;01mand\u001B[39;00m input_name \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;129;01mand\u001B[39;00m has_nan_error:\n\u001B[0;32m    155\u001B[0m     \u001B[38;5;66;03m# Improve the error message on how to handle missing values in\u001B[39;00m\n\u001B[0;32m    156\u001B[0m     \u001B[38;5;66;03m# scikit-learn.\u001B[39;00m\n\u001B[0;32m    157\u001B[0m     msg_err \u001B[38;5;241m+\u001B[39m\u001B[38;5;241m=\u001B[39m (\n\u001B[0;32m    158\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m does not accept missing values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    159\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m encoded as NaN natively. For supervised learning, you might want\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    169\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m#estimators-that-handle-nan-values\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m    170\u001B[0m     )\n\u001B[1;32m--> 171\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(msg_err)\n",
      "\u001B[1;31mValueError\u001B[0m: Input X contains NaN.\nPolynomialFeatures does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "#your code here\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "489a1499",
   "metadata": {},
   "source": [
    "\n",
    "### 1.6 Standardize your design matrix from Question 1.5 using `StandardScaler()`, and store the result as a Pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0838eebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "scaler = StandardScaler()\n",
    "X_scaled = pd.DataFrame(scaler.fit_transform(X_poly), columns=poly.get_feature_names(X.columns))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2821829e",
   "metadata": {},
   "source": [
    "## Question 2: /7 pts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "065a772c",
   "metadata": {},
   "source": [
    "### 2.1 Split the data into training and test sets. Hold out 30% of observations as the test set. How many observations are in your training dataset? What is the average value of the target variable in the training dataset (rounded to 2 decimal places)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfe16088",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e97106",
   "metadata": {},
   "source": [
    "## Question 3: /23 pts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e1e843",
   "metadata": {},
   "source": [
    "### 3.1 Create a SciKit Learn `Ridge` regression object. Train it on the training data using an `alpha` of $4.0$ and do fit the intercept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7aa3cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc897ff",
   "metadata": {},
   "source": [
    "### 3.2 Now use `RidgeCV` to find the best `alpha` for the penalty term through a 5-fold cross-validation. As input for `alpha`, your code must try integer values from 30 to 50 inclusive. Report the `alpha` that yields the smallest loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c84bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a89b5d",
   "metadata": {},
   "source": [
    "### 3.3 Fit a `Ridge` regression on the training data with the best `alpha` found in the previous question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16da9e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9db12d9",
   "metadata": {},
   "source": [
    "### 3.4 Fit a simple `LinearRegression` without any penalty using the training data (again, `fit_intercept=True`). Compare the regression coefficients obtained in questions 3.1, 3.3 and 3.4. How do they compare?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b37a30c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47c04c40",
   "metadata": {},
   "source": [
    "#### YOUR ANSWER HERE\n",
    "\n",
    "Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600cf777",
   "metadata": {},
   "source": [
    "\n",
    "### 3.5 Use your trained linear regression models in Q3.3 and Q3.4 to predict over the test set and print the median of their perdictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "538d0546",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58a650cb",
   "metadata": {},
   "source": [
    "## Question 4: /25 pts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e235cb4",
   "metadata": {},
   "source": [
    "### 4.1 Fit a Lasso regression to the train dataset using lasso_path(). Show the full path of the first 20 coefficients of the Lasso regression. Include eps=8e-3 and n_alphas=50. Describe the trends you see in the figure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bd44ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f68d84",
   "metadata": {},
   "source": [
    "### 4.2 Use Scikit Learn's cross-validated LASSO to automatically search for the best alpha of the LASSO regression on the training set with intercept. Include arguments `eps=8e-3`, `n_alphas=30`, `tol=0.001`, `cv=5`, and `random_state=seed`. Report the best tuning parameters and the number of coefficients that the model shrinks to zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d96736cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97093359",
   "metadata": {},
   "source": [
    "\n",
    "### 4.3 Use Scikit Learn's cross-validated ElasticNet to automatically search for the best tuning parameters of the ElasticNet regression (with intercept) on the training set. Include the same arguments as in question 4.2 as well as `l1_ratio=[0.7, 0.9, 0.95, 0.99, 1]`. Report the best tuning parameters. Is the ElasticNet regression model equivalent to the Lasso regression? Briefly describe how they defer and under what circumstances they become the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a8d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7fdd82a",
   "metadata": {},
   "source": [
    "#### YOUR ANSWER HERE\n",
    "\n",
    "Here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf52d72",
   "metadata": {},
   "source": [
    "## Question 5 : /16 pts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ff07cf4",
   "metadata": {},
   "source": [
    "### 5.1 Use `SequentialFeatureSelector()` to conduct forward selection for the features of the Ridge model tuned in Q 3.3. Include the argument `n_features_to_select=20`. Report the indices of the selected features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18a57650",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfac786b",
   "metadata": {},
   "source": [
    "### 5.2 Fit a regular `LinearRegression` (with `fit_intercept=True`) on the training set using the selected features from the previous question. Print the first 3 coefficients of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd0af24",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49f2f7c",
   "metadata": {},
   "source": [
    "## Question 6: /9 pts\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a675e822",
   "metadata": {},
   "source": [
    "### 6.1 Make predictions on the test set using models from questions 3.3, 4.2, 4.3, and 5.2, respectively. Create a DataFrame with  predicted values obtained from the different models. Name the columns of the dataframe consistent with the names used for the models, or their question number. Display the first 5 rows of this dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e45d5aec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e175b5",
   "metadata": {},
   "source": [
    "### 6.2 Use `mean_squared_error` as your scorer to assess the performance of the different models (those reported in the previous question) based on all the predicted values over test set. Based on this scorer which model is the best?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2a5e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18fb8c15",
   "metadata": {},
   "source": [
    "#### YOUR ANSWER HERE\n",
    "\n",
    "Here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
