{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm\n",
    "\n",
    "## Student ID: XXXXXXXXX (XX / 100)\n",
    "\n",
    "## General comments\n",
    "\n",
    "This Midterm integrates knowledge and skills acquired in the first half of the semester, especially in the first six Assignments. You are allowed to use any document and source on your computer and look up documents on the internet. **You are NOT allowed to share documents, or communicate in any other way with people inside or outside the class during the midterm.** To finish the midterm in the alloted 3 hrs, you will have to work efficiently. **Read the entirety of each question carefully.** You need to be signed into the Midterm Zoom session during the entire midterm with your video on and pointed at yourself. \n",
    "\n",
    "You need to submit the midterm by the due date (13:30) on OWL in the Test & Quizzes / Midterm section where you downloaded the data set and notebook. Late submission will be scored with 0 pts, unless you have received extra accommodation. To avoid technical difficulties, start your submission at latest five to ten minutes before the deadline. To be sure, you can also submit multiple versions (before the deadline) - only the latest version will be graded.  \n",
    "\n",
    "Most questions demand a **written answer** - answer these in a full English sentence. \n",
    "\n",
    "For your Figures, ensure that all axes are labeled in an informative way. \n",
    "\n",
    "Ensure that your code runs correctly by choosing \"Kernel -> Restart and Run All\" before submitting. \n",
    "\n",
    "### Additional Guidance\n",
    "\n",
    "If at any point you are asking yourself \"are we supposed to...\", *write your assumptions clearly in your exam and proceed according to those assumptions.*\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preliminaries\n",
    "# Sets up the environment by importing \n",
    "# pandas, numpy, matplotlib, searborn, sklearn, scipy.\n",
    "# No other packages are allowed in solving the modterm.   \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import sklearn as sk \n",
    "import scipy \n",
    "\n",
    "# Get individual functions from \n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error, r2_score, mean_squared_error\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, precision_recall_curve, auc, plot_roc_curve\n",
    "from scipy.stats import norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set\n",
    "\n",
    "In 2007, Paulo Cortés and Aníbal Morais, at the University of Minho in Portugal, studied the use of data science techniques to tackle the challenging problem of forest fire prediction. They described their work as follows:\n",
    "\n",
    "\"Forest fires are a major environmental issue, creating economical and ecological damage while endangering human lives. Fast detection is a key element for controlling such phenomenon. To achieve this, one alternative is to use automatic tools based on local sensors, such as provided by meteorological stations.\" \n",
    "\n",
    "In general, forest fire risks are measured using the Canadian Forest Fire Weather Index for rating forest fire danger (FWI). You are provided with a dataset from the Montesinho natural park, from the Tras-os-Montes northeast region of Portugal. The data used in the experiments was collected from January 2000 to December 2003. At a daily basis, every time a forest fire occurred, several features were collected such as the time, date, spatial location within a 9×9 grid, the type of vegetation involved, four components of the FWI system, the total burned area, and several weather observations (e.g. wind speed) recorded at the time of the event. The specific variables available to you are:\n",
    "\n",
    "1. X - x-axis spatial coordinate within the Montesinho park map: 1 to 9\n",
    "2. Y - y-axis spatial coordinate within the Montesinho park map: 2 to 9\n",
    "3. month - month of the year: 1 to 12 (categorical)\n",
    "4. day - day of the week: 1 to 7 (categorical)\n",
    "5. FFMC - FFMC (Rain, Temperature, Wind and Humidity relationship) index from the FWI system: 18.7 to 96.20\n",
    "6. DMC - DMC (Rain, Temperature,and Humidity relationship) index from the FWI system: 1.1 to 291.3\n",
    "7. DC - DC (Rain and Temperature relationship) index from the FWI system: 7.9 to 860.6\n",
    "8. ISI - ISI (Wind speed risk) index from the FWI system: 0.0 to 56.10\n",
    "9. temp - temperature in Celsius degrees: 2.2 to 33.30\n",
    "10. RH - relative humidity in %: 15.0 to 100\n",
    "11. wind - wind speed in km/h: 0.40 to 9.40\n",
    "12. rain - outside rain in mm/m2 : 0.0 to 6.4\n",
    "13. area - the burned area of the forest (in ha): 0.00 to 1090.84 (**target variable**).\n",
    "\n",
    "With this information, answer the following questions using your knowledge from the course.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 1: Predicting the size of the burned area (30 pts)\n",
    "\n",
    "### Question 1.1: Data loading (X / 4 pts)\n",
    "\n",
    "Load the data and present the basic descriptive statistics for all variables. How many cases are there? Are there any null values?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.2: Data cleaning (X / 3 pts)\n",
    "\n",
    "The variables 'month' and 'day' are numerical. Transform these two so they are dummy coded. Because in this task we are using non-regularized models, drop the first column for each variable. Written answer: How many new variables have you just created?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.3: Data visualization (X / 5 pts)\n",
    "\n",
    "Create a joint plot of the distribution between the temperature and the area. In the same plot, show the marginal distributions of these two variables seperately. Written answer: How would you characterize the distribution of `Area`? Is there a relationship between temperature and area? If yes, how would you describe it? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.4 Logarithmic transformation (X / 3 pts)\n",
    "\n",
    "Given the distribution of the area variable, the original researchers proposed **using the logarithm of the area instead of the original variable**. Calculate this new target variable (name it ```LogArea```) so that $LogArea = log(area + 1)$.\n",
    "\n",
    "Now, create a joint plot again of the distribution between the temperature and your newly created variable ```LogArea```. Written answer: Do you see any relationship more clearly now? Do you think it was a good idea to make this transformation? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 1.5: Regression comparison (X / 15 pts)\n",
    "\n",
    "Now we will compare the performance of a linear regression model with the two different target variables. For this:\n",
    "\n",
    "a. Create a train / test split using 30% of the data as a test set. Use a `random_state` of 1 if using `train_test_split` (2 pts).\n",
    "\n",
    "b. Create a regression of all predictor variables (X), including the dummy-coded variables from Q1.2, versus the area variable (y) without transformation on the training data only (6 pts).\n",
    "\n",
    "c. Create a regression of all variables (X), including the dummy-coded variables from Q1.2, versus the log-transformed area variable (y) on the training data only (3 pts).\n",
    "\n",
    "d. Calculate and report the mean absolute error of both models when predicting the variable `area` in the test set. Make sure you transform the prediction of the model in c) from log(area) to an area first! Written answer: Which one is the most accurate model? Why do you think this happens? (4 pts - 2pts for results, 2 pts for written answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 2: Build a classifier to distinguish major and minor forest fires (35 points)\n",
    "\n",
    "For the second task we will study whether we can differentiate a major from a minor forest fire. For this, we first need to decide what exactly a major forest fire is and test whether we can significantly differentiate one versus the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.1: Building the new target variable. (X / 5 pts)\n",
    "\n",
    "Let's start by defining a major burn as any forest fire that burns more than 1 ha. Reimport the data and now create the dummy variables without dropping any of the columns (as we will regularize our model). Then create a binary variable that represents whether a forest fire is larger than one ha or not.\n",
    "\n",
    "Then create a set of histograms or distribution plots (you can use for example `displot` in seaborn) that shows the the distribution of the minor and major forest fires as a function of all major predictor variables (`X`, `Y`, `FFMC`, `DMC`, `DC`, `ISI`, `temp`, `RH`, `wind`, `rain`). Skip the dummy variables. \n",
    "\n",
    "Written answer: What percentage of fires are classified as major this way? Looking at the plots, do you think you can create a model differentiating between major and minor forest fires?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.2: Ridge regression (X / 10 pts)\n",
    "\n",
    "Now we need to put your theory from the previous written answer to the test. Your objective is to train a logistic regression using your newly created variable. For this:\n",
    "\n",
    "a. Create a train / test split of the dataset reserving 30% for the test set. Use `random_state = 1` if using `train_test_split`. [2 pts]\n",
    "\n",
    "b. Create a pipeline that first z-standardizes (makes it have mean zero and a standard deivation of 1) your training data and then trains a logistic regression **using a tolerance of 0.001 and sufficient iterations for the model to converge. Use a C of 0.006 (Ridge parameter).** [2pts]\n",
    "\n",
    "c. Fit the model to the training data. Show the coefficients and what variable they are related to. [3pts]. Written answer: Based on these coefficients, which variable has the largest influence on the prediction[1pt]? In the context of this model, does wind speed lead to lower or higher risk of major fires[1pt]? How important is the influence of the day of the week and which of the days of the week are the riskiest[1pt]?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.3: Validating your model. (X / 5 pts)\n",
    "\n",
    "Now we will begin validating our model. Apply your results to the test set you created. Report the accuracy (leaving the default cutoff of 0.5 on the prediction), plot the ROC curve, and calculate the AUC. Comment on the performance of your model. How well does the model differentiate between major and minor forest fires?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 2.4: A bootstrapped AUC measure. (X / 15 pts)\n",
    "\n",
    "Now we want to estimate a bootstrapped AUC. Create a bootstrap measure over the training set, with 100 runs, training your pipeline and calculating the AUC each time. Be careful that you must calculate the AUC in the corresponding \"test set\" of the bootstrapped sample, that is, calculate the ROC curve over the elements that were not selected in the sample.\n",
    "\n",
    "A useful function for this is [Panda's `isin` function](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.Index.isin.html) that returns a vector showing if the index is in a set. You must select the elements of your original data that are NOT in the index of your sample. You use this function in the following way: if you have a dataset called ```sample``` with a sample of the original dataset called ```data```, then the following code creates a test set with the elements in ```data``` that are not in ```sample```:\n",
    "\n",
    "```\n",
    "test = data[~data.index.isin(sample.index.values)]\n",
    "```\n",
    "\n",
    "After you have calculated the bootstrapped AUC, plot the distribution you obtained and calculate the standard deviation of the bootstrap samples. Assuming a normal distribution of the estimated AUC, construct a 95% confidence for the AUC measure you obtained in Question 2.3. (hint: on a standard normal distribution 95% of the probability mass is between -1.96 and +1.96 std deviations). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and run the bootstrap [7pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the distribution of AUC values [2 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and show the AUC confidence intervals [6 pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Task 3: Build a regularized predictive model for temperature (35 points)\n",
    "\n",
    "Because temperature is such an important determinant of fire danger, in this task we are trying to build a good seasonal model of the temperature observed in the park. \n",
    "\n",
    "### Question 3.1 (X / 5 pts)\n",
    "Reload the data set, so you get rid of your dummy variables and regain the original `month` and `day` variables. Make a `seaborn.stripplot` of Month on the x-axis and temperature on the y-axis. \n",
    "Written answer: What type of function would describe the relationship between average temperature and the month of the year best? Would a quadratic polynomial do well? For which month do you have the least observations? How many? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.2 (X / 5 pts)\n",
    "To model the relationship, you decide to encode the month of in terms of a fourier set (see lecture 6). \n",
    "\n",
    "Write a function `FourierExpansion(x, order)`, which takes a variable `x` scaled between 0 and 1, and returns a design matrix of size order times 2 columns ($order \\times 2$). The first 2 columns should be set to: \n",
    "\n",
    "$sin(2 \\pi x)$\n",
    "\n",
    "$cos(2 \\pi x)$\n",
    "\n",
    "The next two columns: \n",
    "\n",
    "$sin(4 \\pi x)$\n",
    "\n",
    "$cos(4 \\pi x)$\n",
    "\n",
    "$...$\n",
    "\n",
    "and the last two columns: \n",
    "\n",
    "$sin(2 order \\pi x)$\n",
    "\n",
    "$sin(2 order \\pi x)$\n",
    "\n",
    "\n",
    "Test the function by calling it with \n",
    "\n",
    "`x=(np.array(range(12))+1)/12` and `order = 6`. \n",
    "\n",
    "Plot the design matrix as an image. \n",
    "\n",
    "Written answer: \n",
    "Which of the 12 columns in the design matrix can / should be removed before fitting? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.3 (X / 5 pts)\n",
    "Now generate two different design matrices (models) to model the variation of temperature across the month of the year. \n",
    "* Model 1: use your function from Question 3.2 to generate a Fourier set (order 6) for month. Make sure you scale month between 0 and 1 before submitting it to the function. \n",
    "* Model 2: use `pd.get_dummies` to generate a dummy or one-hot encoding of month. Because we will use this for an unregualized model, make sure you drop the first column. \n",
    "\n",
    "Fit the two unregularized linear models to the data and report the mean-square-error and the $R^2$ value for both models. If you have done everything correctly, the mse and $R^2$ values should be identical. Why is this? Hint: How many columns are in each design matrix and how many months in the year are there? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.4 (X / 12 pts)\n",
    "a) Modify your dummy encoding model (Model 2 from Question 3.3), by not dropping the first column. This design matrix should have 12 regressors now. \n",
    "\n",
    "b) Build a pipeline, which first applies z-Standardization to each column of the design matrix and then fits an L2-regularized linear model. \n",
    "\n",
    "c) Do a grid search, varying the regularization parameter between $exp(-5)$ and $exp(4)$ in 30 steps (linearly spaced in $log(\\lambda)$ for the fourier-encoded model, (Model 1 from Question 3.3). Evaluate each setting of $\\lambda$ using the mean-squared-error with 30-fold cross-validation. Plot the average validation error (y-axis) against $log(\\lambda)$ (x-axis). (*Hint: If you did not succeed in generating a fourier feature set, use `polynomialFeatures` to make a polynomial feature set for month of order 11.*)\n",
    "\n",
    "d) Repeat c), this time using the dummy-coded feature set (modified Model 2 from question 3.4, part a). Plot the average validation error with a different color in the same plot as part c), so you can compare the models. \n",
    "\n",
    "e) Report the $\\lambda$ value for each model that gives the best validation error. \n",
    "\n",
    "Written answer: What do you see? Why don't the two models behave the same way, as they did Question 3.3? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a. Generate new dummy encoding [1pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# b. Build the pipeline with z-standardization and L2-Regularization [1pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# c. Grid search [5pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# d. Repeat [3 pts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# e. Report the best regularization parameter [2pts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Question 3.5 (X / 8 pts)\n",
    "Now make a third model that has only a Fourier set of order 3 (or a polynomial feature set of order 5, if you did not succeed in solving question 3.2). This is Model 3. \n",
    "Plot the same grid-search as in question 3.4 on this model and plot the validation error as a function of the regularization parameter. In the same plot, add the validation error for Model 1 (Question 3.3/ 3.4). \n",
    "\n",
    "Written answer: \n",
    "You should see that Model 3 provides a lower validation error for any setting of the ridge coefficient than the Model 1 considered in Question 3.4. Which one of the model is the more complex model? Which one is more likely to overfit the data? Why can't L2-regularization fix this overfitting problem - i.e., why does the validation error for Model 1 not approach the validation error for Model 3?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Written answer:**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('my_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "3420a8792bbc8a921cecec9f5e200567f9d5b83365a03086ee32a665b051d9eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
