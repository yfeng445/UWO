{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm\n",
    "# Name: XXXXXXXX\n",
    "# Student ID number: XXXXXXXXX <font color = 'green'> (XX / 100) </font>\n",
    "\n",
    "## General comments \n",
    "This Midterm integrates knowledge and skills acquired in the first half of the semester, especially in the 6 Assignments. You are allowed to use any document and source on your computer and look up documents on the internet. **You or not allowed to share documents, or communicate in any other way with people in real time during the midterm.** To finish the midterm in the alloted time, you will have to work efficiently. **Read the entirety of each question carefully.**\n",
    "\n",
    "## Logistics\n",
    "\n",
    "The midterm is officially 2 hours (to fit within the timeslot) and is designed to be completable within 2 hours. However, we will accept submissions up to 3 hours after the start time without penalty. Midterms submitted more than 3 hours after the start time will not be marked. You may submit an unlimited number of times in the 3-hour window; I encourage you to submit as often as you want.\n",
    "\n",
    "Questions during the midterm may be sent by OWL Message to the Instructor Role. Only clarification questions will be addressed, and clarification responses will only be sent to the entire class to be fair. If I decide a clarification is not needed, I will not send any response. I will have OWL override and copy any clarifications to your e-mail, so have that open as you write.\n",
    "\n",
    "### Additional Guidance\n",
    "\n",
    "If at any point you are asking yourself \"are we supposed to...\", then *write your assumptions clearly in your exam and proceed according to those assumptions.*\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Preliminaries\n",
    "# Sets up the environment by importing \n",
    "# pandas, numpy, matplotlib.\n",
    "\n",
    "### YOU MAY ADD ADDITIONAL IMPORTS IF YOU WISH\n",
    "### HERE OR IN ANY OF THE CELLS BELOW\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "import scipy.stats as ss\n",
    "from scipy.optimize import minimize\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "import sklearn as sk \n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.metrics import make_scorer, mean_absolute_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Task 1 - Beta Regression ( X / 30 pts )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this task you will develop a maximum likelihood regression model where both the target values and the predictions are strictly between between 0 and 1. This scenario arises often when data represent a percentage or proportion, but where the \"counts\" that make up the proportion are unknown. For example, measurements of blood oxygen saturation range from 0 (completely desaturated) to 1 (completely saturated.)\n",
    "\n",
    "There are four main parts to this task.\n",
    "\n",
    "1. Choose and implement an appropriate function that maps inputs $X$ and parameters $b$ to a predictive mean $\\mu$.\n",
    "\n",
    "2. Define an appropriate log likelihood of the data given parameters.\n",
    "\n",
    "3. Define the negative log likelihood function for beta regression.\n",
    "\n",
    "4. Apply your regression model to an example dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1 Part 1 - Mean Function ( X / 5 pts)\n",
    "\n",
    "Complete the definition of the function below. Your function must take an $n \\times m$ matrix $X$ and an $m \\times 1$ vector of parameters and produce a prediction for each row of $X$ that is between 0 and 1. You can use a built-in function or define \"from scratch\"; both are fine."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(X,b):\n",
    "    # Put your code below\n",
    "\n",
    "    \n",
    "\n",
    "    return predictions\n",
    "    \n",
    "# Example:    \n",
    "\n",
    "X = np.array([[1,2],[1,3],[1,4]])\n",
    "b = np.array([[1],[-1]])\n",
    "\n",
    "predict(X,b)\n",
    "\n",
    "# Should produce:\n",
    "# array([[0.26894142],\n",
    "#        [0.11920292],\n",
    "#        [0.04742587]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1 Part 2 - Likelihood Function ( X / 5 pts )\n",
    "\n",
    "Your next task is to write a negative log likelihood function for this regression model. We will use the \"beta distribution\" to define the likelihood, because it is a continuous distribution that ranges from 0 to 1.\n",
    "\n",
    "The standard beta distribution takes two parameters, $a$ and $b$. The larger $a$ is, the more the distribution is \"pushed\" toward 1, and the larger $b$ is, the more it is \"pushed\" toward 0. The following code will let you see how this works; you can try different $a$ and $b$ if you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code is just so you can see what the beta distribution looks like.\n",
    "a = 8\n",
    "b = 5\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "x = np.linspace(0,1,100)\n",
    "ax.plot(x, ss.beta.pdf(x, a, b), 'r-', lw=5, alpha=0.6, label='beta pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since in regression we typically model the mean as a function of inputs $X$, it is more convenient to parameterize the beta distribution in terms of the mean $\\mu \\in (0,1)$ and a \"concentration\" $\\phi > 0$, where\n",
    "\n",
    "$$a = \\mu \\cdot \\phi$$\n",
    "$$b = (1 - \\mu) \\cdot \\phi$$\n",
    "\n",
    "Implement the beta **log likelihood** function below, which takes a vector of data $y$, a vector of means $\\mu$, and a scalar (single number) concentration $\\phi$, and produces the log likelihood of $y$, assuming each element comes from a beta distribution with parameters given by the corresponding element in $\\mu$ and concentration $\\phi$.\n",
    "\n",
    "BIG HINT 1: Recall from the notes that the log likelihood of a single observation of a continuous random variable given specified parameters is the log of the probability density function for that random variable evaluated at the observation.\n",
    "\n",
    "BIG HINT 2: You can find useful helper functions here https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.beta.html which you can access in the form 'ss.beta.functionname(...)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def betaloglik(y,mu,phi):\n",
    "    # Fill in your code below\n",
    "\n",
    "    return bll\n",
    "    \n",
    "# Example:\n",
    "betaloglik(np.array([0.2,0.3,0.4,0.5,0.6]), np.array([0.2,0.3,0.4,0.5,0.6]), 100)\n",
    "# should give\n",
    "# 10.719823849145598"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1 Part 3 - Regression Negative Log Likelihood ( X / 5 pts )\n",
    "Now you will define the regression negative log likelihood, where the values for $\\mu$ come from regression predictions. This will make it a bit tricky because the function must accept $\\phi$ and $b$ together as one parameter so that we can later use the minimize function. Skeleton code and test code is provided to take care of this; just fill in the missing pieces."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def betaregnegloglik(theta,X,y):\n",
    "    # theta contains phi followed by beta\n",
    "    phi = theta[0]\n",
    "    beta = theta[1:]\n",
    "    # Fill in with your code below\n",
    "\n",
    "    return bnll\n",
    "\n",
    "X = np.array([[1,2],[1,3],[1,4]])\n",
    "b = np.array([[1],[-1]])\n",
    "y = np.array([[0.2],[0.4,[0.6]]])\n",
    "\n",
    "phi = np.array([[10]]) # Makes phi a 1 by 1 array\n",
    "testtheta = np.r_[phi,b] # Stack phi on top of b to get theta\n",
    "betaregnegloglik(testtheta,X,np.array([0.2,0.4,0.6])) # Compute\n",
    "# Should produce 16.50271105637038"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1 Part 4 - Maximum Likelihood Beta Regression ( X / 10 pts )\n",
    "\n",
    "This example comes from the prominent statistical analysis software *stata*. The manual page for beta regression in stata is here:\n",
    "\n",
    "[ https://www.stata.com/manuals/rbetareg.pdf ]\n",
    "\n",
    "The following example uses synthetic (i.e. made-up) data about the pass rate at different schools.\n",
    "\n",
    "The variable to predict is the school-wide pass rate given in the *prate* variable, which must be between 0 and 1.\n",
    "\n",
    "The predictive variables are *freemeals* which denotes the proportion of students receiving free meals at school, *pdonations* which denotes the level of donations received by the school, and *summer* which indicates whether or not the school puts on a summer instruction program for students."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_stata(\"sprogram.dta\")\n",
    "df = pd.get_dummies(df, drop_first = True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complete the code in the cell below to find the maximum likelihood coefficients for beta regression. Include an intercept. Use the 'Nelder-Mead' method, which does not require a gradient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function definition - 10 pts\n",
    "def betaregmaximumlikelihood(X,y):\n",
    "    phistart = np.ones((1,1))\n",
    "    bstart = np.zeros((X.shape[1],1))\n",
    "    thetastart = np.r_[phistart, bstart] # Stack one on top of the other\n",
    "    \n",
    "    #Fill in function here\n",
    "\n",
    "    return (b,phi)\n",
    "\n",
    "# Set up X and y here\n",
    "\n",
    "betaregmaximumlikelihood(X,y)\n",
    "\n",
    "# Should produce\n",
    "# (array([ 1.17499393, -0.45635901,  0.04496522,  0.55600845]), 10.755643379822022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 1 Part 5 - Interpreting the Beta Regression Model - (X / 5) pts\n",
    "\n",
    "Based on the regression coefficients, for each of the input variables in the dataset, explain how a change in its value is associated with a change in the estimated proportion of students who attain a passing grade. (You can just explain the direction of the change rather than the magnitude.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Write your answer below:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Task 2 - Classification and Evaluation (X / 40 pts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data set \n",
    "The Child Health and Development Studies investigate a range of topics. One study considered all pregnancies between 1960 and 1967 among women in the Kaiser Foundation Health Plan in the San Francisco East Bay area.\n",
    "\n",
    "The data frame contains the following data, where each row represents a baby:\n",
    "\n",
    "- bwt: birth weight (ounces)\n",
    "- gestation: length of pregnancy (days)\n",
    "- parity: 0 if mom's first baby, 1 if not mom's first baby\n",
    "- age: mom's age (years)\n",
    "- height: mom's height (inches)\n",
    "- weight: mom's weight (pounds)\n",
    "- smoke: 0 if mom is nonsmoker, 1 if mom is smoker\n",
    "\n",
    "Note that babies that are born with a gestation of less than 37 weeks (259 days) are considered pre-term and are at a higher risk for health complications.\n",
    "\n",
    "Overall this task will focus on building a good predictive model whether a baby will be pre-term or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loads the data set and drops the case number, as well as any Missing observations: \n",
    "df = pd.read_csv('babies.csv').drop('case',axis='columns').dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2 Part 1 - Warm up (X / 10 pts)\n",
    "In the cell below, write code that does the following:\n",
    "- Prints the number of observations in the dataset\n",
    "- Prints the number of variables in the dataset (all variables regardless of whether they are a predictor or label or neither)\n",
    "- Adds a new variable to the dataset called 'preterm' that is 1 if the baby is pre-term and 0 otherwise\n",
    "- Prints the number of pre-term babies in the dataset\n",
    "- Prints the first few rows of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Warm-up code\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2 Part 2 - Logistic Regression Model (X / 5 pts)\n",
    "\n",
    "Your next task is to build a model that predicts whether a baby will be pre-term or not using the characteristics of the mother (assuming they are measured prior to birth.) In the cell below, estimate a logistic regression model for pre-term status given these characteristics and print out its coefficients and intercept. Do not use a penalty for learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for building a logistic regression model\n",
    "\n",
    "\n",
    "# Actual Coefficients: [[-0.35512466  0.00058173 -0.04752664  0.00488872  0.13200691]]\n",
    "# Actual Intercept: [0.00343994]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2 Part 3 - In-sample Evaluation ( X / 5 pts )\n",
    "\n",
    "In the cell below, compute the accuracy and the AUROC for your classifier using the training data and print these out. Then explain why the observed accuracy is much higher than the AUROC score in this case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for computing accuracy and AUROC \"in-sample\", that is, on the training data.\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T2P3 Written answer: Explain in this cell why the observed accuracy is much higher than the AUROC score in this case.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2 Part 4 - Out-of-sample Evaluation ( X / 10 pts )\n",
    "In the cell below, compute the accuracy and the AUROC for your classifier using 10-fold stratified cross-validation and print these out. Then explain:\n",
    " 1) Why stratified cross-validation is appropriate for this dataset and\n",
    " 2) Any differences you see between these CV-based estimates from your in-sample estimates from Part 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code for computing accuracy using stratified K-fold CV\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "# https://scikit-learn.org/stable/modules/cross_validation.html#cross-validation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### T2P4 Written answer\n",
    "\n",
    "1. Explain why stratified cross-validation is appropriate for this dataset\n",
    "\n",
    "*Answer here*\n",
    "\n",
    "2. Explain why these estimated values are different from those in Part 3.\n",
    "\n",
    "*Answer here*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 2 Part 5 - Discussing Evaluation ( X / 10 pts )\n",
    "\n",
    "There are many ways to evaluate a classifier. For each of the four that you just computed, (training accuracy, training AUROC, CV accuracy, CV AUROC) explain in a sentence or two whether it is useful for assessing the generalization performance of your learned model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Written answer: Explain here.\n",
    "\n",
    "1. Training accuracy\n",
    "\n",
    "2. Training AUROC\n",
    "\n",
    "3. CV accuracy\n",
    "\n",
    "4. CV AUROC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "# Task 3 - Model Selection, Regularization (X / 30 pts)\n",
    "\n",
    "In this section we will use the same data but we will predict **birth weight** from the other variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3 Part 1 - Linear Regression (X / 5 pts)\n",
    "\n",
    "In the cell below, provide code to use 10 fold cross-validation to assess the performance of a linear regression model to predict birth weight from the other variables in terms of its root mean squared error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here for linear model, 10-fold CV.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3 Part 2 - Linear Regression, Complex Model (X / 5 pts)\n",
    "Use the following polynomial feature expansion pipeline component, called `preprocess`, to build a linear regression model to predict the birth weight (bwt). Read through the comments to make sure you understand what is happening. You do not need to modify any of this code.\n",
    "\n",
    "**In the cell after this giant one full of code** provide code to use the preprocessor to build a linear regression model to predict birthweight based on the transformed data, and evaluate it using root mean squared error from 10-fold cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dummies, dropping the first category\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler, PolynomialFeatures, OneHotEncoder\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "# Get all predictors\n",
    "X = df.loc[:, ['gestation','parity','age','height','weight','smoke']]\n",
    "# Get bwt - target variable\n",
    "y = df.loc[:, ['bwt']]\n",
    "\n",
    "# List categorical features (no scaling, no polynomial-ifying)\n",
    "categorical_features = ['parity','smoke']\n",
    "# List numeric (continuous) features\n",
    "numeric_features = [column for column in X.columns if all(cat_var not in column for cat_var in categorical_features)]\n",
    "\n",
    "# Collect the names of the new features that will be created by the polynomial transformer\n",
    "numeric_names = PolynomialFeatures(5, include_bias=False).fit(X.loc[:,numeric_features]).get_feature_names(numeric_features)\n",
    "all_names = numeric_names\n",
    "# Add on the categorical feature names\n",
    "all_names.extend(categorical_features)\n",
    "# This holds all the feature names in the new space\n",
    "all_names = np.asarray(all_names)\n",
    "\n",
    "# This transformer will scale, then transform to polynomial features, then re-scale!\n",
    "# To be applied to continuous variables only.\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('scaler', StandardScaler()),\n",
    "    ('poly', PolynomialFeatures(5, include_bias=False)),\n",
    "    ('scaler2', StandardScaler())])\n",
    "\n",
    "# This transformer pretty much does nothing. It's for the categorical variables.\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('onehot', OneHotEncoder(drop='first'))])\n",
    "\n",
    "# Make a column transformer with two transformers, one for numeric and one for categorical.\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features)])\n",
    "\n",
    "# You can now use the \"preprocess\" transformer in any pipeline you want, worry-free. \n",
    "# Always apply it to the original data X.\n",
    "\n",
    "# Example:\n",
    "Xtransform = preprocess.fit_transform(X)\n",
    "Xtransformdf = pd.DataFrame(Xtransform)\n",
    "Xtransformdf.columns = all_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here for higher-dimensional model, 10-fold CV, report RMSE\n",
    "\n",
    "# If you use a pipeline, it should have just two steps, the preprocess step (defined above) and the regression step.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3 Part 3 - Feature selection with the LASSO ( X / 10pts )\n",
    "\n",
    "Based on the training set alone, use 10-fold cross-validation to determine the best value for an L1 penalty (i.e., the LASSO.) **Despite what you may have read in A6 instructions, you should fit an intercept for this part (as in all your other models on the midterm.) This is an issue we will discuss more in the class going forward.**\n",
    "\n",
    "Vary the penalty coefficient alpha from 0.1 to 1.0 in 20 steps. Make a scatter plot of the cross-validation-based RMSE against the the LASSO regularization parameter. Print out the regularization parameter value that you think is best, and print out the CV-based root mean squared error associated with that choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Code here for LASSO feature selection\n",
    "from sklearn.linear_model import Lasso\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html\n",
    "\n",
    "# Compute scores for different parameter values\n",
    "\n",
    "\n",
    "# Code to extract and plot lambda values and rmse values\n",
    "\n",
    "\n",
    "# Plot RMSE against regularization parameter (fill in)\n",
    "myLambdas = None\n",
    "myRMSEs = None\n",
    "sns.scatterplot(x=myLambdas, y=myRMSEs)\n",
    "\n",
    "# Print out your chosen regularization parameter value (fill in)\n",
    "mylam = None\n",
    "print(f\"My chosen regularization value is {mylam}\")\n",
    "\n",
    "# Print out the RMSE for the model with the chosen lambda value (fill in)\n",
    "myRMSE = None\n",
    "print(f\"My best RMSE score is {myRMSE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Task 3 Part 4: Interpreting Results ( X / 5 pts )\n",
    "Answer the questions in the cells below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### How many features (including the intercept) are used in your model from Part 1, Part 2, and by your \"best\" model from Part 3? If you need to run some code to find this out, you can use the cell below. (3 pts)\n",
    "1. Number of features in Part 1 model: \n",
    "2. Number of features in Part 2 model: \n",
    "3. Number of features in best Part 3 model: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Consider the models you built in Part 1, Part 2, and the best model from Part 3. If you had to choose one of these models to deploy, which one would you choose? Explain your answer in 2 or 3 sentences. (2 pts)\n",
    "\n",
    "## Your answer\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "**You're done! As always, double-check your work by re-running the notebook from scratch.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 ('my_env': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "3420a8792bbc8a921cecec9f5e200567f9d5b83365a03086ee32a665b051d9eb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
