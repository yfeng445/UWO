{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0kZDDhGoOJm"
      },
      "source": [
        "# Final Exam - DS 3000B 2022\n",
        "\n",
        "## Student ID: XXXXXXXXX\n",
        "\n",
        "## General comments\n",
        "\n",
        "This Final integrates knowledge and skills acquired during the whole semester. You are allowed to use any document and source on your computer and look up documents on the internet. **You are NOT allowed to share documents, or communicate in any other way with people inside or outside the class during the exam.** To finish the exam in the alloted 4 hrs, you will have to work efficiently. **Read the entirety of each question carefully.**\n",
        "\n",
        "You need to submit the final by the due date (13:00) on OWL in the Test & Quizzes / Final section where you downloaded the data set and notebook. Late submission will be scored with 0 pts, unless you have received special accommodations. To avoid technical difficulties, start your submission at the latest ten minutes before the deadline. To be sure, you can also submit multiple versions - only the latest version will be graded.  \n",
        "\n",
        "Most question demand a **written answer** - answer these in a full English paragraph. \n",
        "\n",
        "For your Figures, ensure that all axes are labeled in an informative way. \n",
        "\n",
        "Ensure that your code runs correctly by choosing \"Kernel -> Restart and Run All\" before submitting. \n",
        "\n",
        "### Additional Guidance\n",
        "\n",
        "If at any point you are asking yourself \"are we supposed to...\", then *write your assumptions clearly in your exam and proceed according to those assumptions.*\n",
        "\n",
        "Good luck!"
      ],
      "id": "V0kZDDhGoOJm"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8OQHvXl9oOJp"
      },
      "outputs": [],
      "source": [
        "# Install UMAP, SHAP and yellowbrick if needed\n",
        "# !pip install umap-learn shap yellowbrick"
      ],
      "id": "8OQHvXl9oOJp"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XcXg-sa9oOJp"
      },
      "outputs": [],
      "source": [
        "## Preliminaries\n",
        "# Sets up the environment by importing \n",
        "# pandas, numpy, matplotlib, searborn, sklearn, scipy.\n",
        "# You should not need any other packages.   \n",
        "\n",
        "# Basics\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Models and metrics\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV, train_test_split, KFold, StratifiedKFold\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.linear_model import LogisticRegressionCV\n",
        "from sklearn.metrics import roc_curve, roc_auc_score, mean_squared_error, confusion_matrix\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from sklearn.linear_model import LinearRegression, Ridge, RidgeCV\n",
        "from sklearn.preprocessing import PolynomialFeatures \n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.compose import ColumnTransformer\n",
        "\n",
        "# tensorflow\n",
        "import tensorflow.keras as keras\n",
        "from tensorflow.keras import models\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "# Clustering\n",
        "from sklearn.cluster import AgglomerativeClustering, KMeans\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import silhouette_samples, silhouette_score\n",
        "from yellowbrick.cluster import KElbowVisualizer, SilhouetteVisualizer\n",
        "from yellowbrick.cluster.elbow import kelbow_visualizer\n",
        "\n",
        "# UMAP\n",
        "import umap\n",
        "\n",
        "# Shap\n",
        "import shap\n",
        "\n",
        "# Plotting\n",
        "import matplotlib.cm as cm\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "%matplotlib inline"
      ],
      "id": "XcXg-sa9oOJp"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kckMn3uEoOJq"
      },
      "source": [
        "## Data set\n",
        "\n",
        "You are given a dataset, `bank_data.csv`, which includes details of a bank's clients and the target variable is a binary variable reflecting whether the client closed their account or they continued to be with the bank (a phenomenon known as **churn**). Banks spend a large amount of resources in keeping their customers, as retaining a customer is much cheaper than capturing a new customer (try to cut your internet and see what happens!). The following variables are given in the dataset:\n",
        "\n",
        "1. Credit_Score: Credit score of the client.\n",
        "2. Age: Age of the client.\n",
        "3. Tenure: Number of years for which the client has been with the bank.\n",
        "4. Balance: Bank balance of the client.\n",
        "5. Number_of_Products: Number of bank products the client is using.\n",
        "6. Has_Credit_Card (binary): 1 if the client has a credit card with the bank, 0 otherwise.\n",
        "7. Active_Member (binary): 1 if the client is an active member, 0 otherwise.\n",
        "8. Estimated_Salary: Estimated salary of the client.\n",
        "9. Closed_Account (binary, target variable): 1 if the client closed their account, 0 otherwise.\n",
        "\n",
        "With this information, execute the following tasks using your knowledge from the course."
      ],
      "id": "kckMn3uEoOJq"
    },
    {
      "cell_type": "code",
      "source": [
        "# Download the data if using online platforms\n",
        "#!gdown https://drive.google.com/uc?id=1Ng3J8joHC7xlGAKyvEtHpXrBzszTu7OZ"
      ],
      "metadata": {
        "id": "lGZSHHdvoScB"
      },
      "id": "lGZSHHdvoScB",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwZhxcFBoOJr"
      },
      "source": [
        "## Task 1 (35 points UG, 25 points PG)\n",
        "Before we start working on a predictive models for whether somebody will leave the company or not, in task 1 we will first build a model to predict the typical credit score of a person given other data. This can be useful as it may be a company wants to target customers within a certain range of risk, or because obtaining the credit scores for all customers may not be cost-effective."
      ],
      "id": "dwZhxcFBoOJr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ebt5WgHzoOJr"
      },
      "source": [
        "### Question 1.1 (20% of task's points):\n",
        "a. Import the dataset and report the data shape. Are there any null values?\n",
        "\n",
        "b. Present the descriptive statistics of the variables.\n",
        "\n",
        "c. Create a distribution plot that shows both the KDE and the histogram of the ```Credit_Score``` variable. **Written answer: What distribution does this variable have?**"
      ],
      "id": "Ebt5WgHzoOJr"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xa6eVej6oOJs"
      },
      "outputs": [],
      "source": [
        "# Your code here. You can add as many cells as you want.\n"
      ],
      "id": "Xa6eVej6oOJs"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CwBE7UqVoOJu"
      },
      "source": [
        "**Written answer here**"
      ],
      "id": "CwBE7UqVoOJu"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dVHlZmGhoOJu"
      },
      "source": [
        "### Question 1.2 (20% of task's points):\n",
        "a. Define X (predictors) and y (target) for your problem. Your target corresponds to the ```Credit_Score``` variable \n",
        "\n",
        "b. Split the data into X_train, X_test, y_train, y_test with `test_size = 0.3` and `random_state = 2022`. Do not use ```Credit_Score```as a predictor in your model.\n",
        "\n",
        "c. Apply a `MinMaxScaler` to both the training and the test data (X_train, X_test)."
      ],
      "id": "dVHlZmGhoOJu"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NWrEUHhWoOJv"
      },
      "outputs": [],
      "source": [
        "# Your code here. You can add as many cells as you want.\n"
      ],
      "id": "NWrEUHhWoOJv"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P7NbBcTboOJw"
      },
      "source": [
        "### Question 1.3 (60% of task's points)\n",
        "\n",
        "Now we will train a couple of models and study their behaviour. \n",
        "\n",
        "a. Train an unregularized linear regression and calculate the MSE error over the test set. Show the coefficients and the variable they refer to.\n",
        "\n",
        "b. Now train a Ridge model, tuning the alpha value between 0 and 1000 in intervals of 50. Use five crossvalidation folds over the train set using a ```KFold``` object with a random seed of 2022 and ```shuffle=True```, you can use ```RidgeCV``` if desired. Show the optimal alpha value, the trained coefficients, and the variables they refer to.\n",
        "\n",
        "c. Create a jointplot of the two predictions over the test set (a scatterplot of the prediction of the model in a and the model in b).  **Written answer: Where are performances different?**.\n",
        "\n",
        "d. Using a normal approximation, calculate the CI for the test set error (squared residuals). **Written answer: Which model performs best? Why do you think this happens? Why can you use a normal approximation for this error? Which model would you use?**"
      ],
      "id": "P7NbBcTboOJw"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "do75g8rRoOJx"
      },
      "outputs": [],
      "source": [
        "# Your code here. You can add as many cells as you want.\n"
      ],
      "id": "do75g8rRoOJx"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y9iX1em1oOJz"
      },
      "source": [
        "**Written answer here.**"
      ],
      "id": "Y9iX1em1oOJz"
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here. You can add as many cells as you want."
      ],
      "metadata": {
        "id": "634jr6dyR8i6"
      },
      "id": "634jr6dyR8i6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Written answer here**"
      ],
      "metadata": {
        "id": "GGIZlP34Txu7"
      },
      "id": "GGIZlP34Txu7"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9s5Ud9GoOJz"
      },
      "source": [
        "## Task 2: Tree-based ensemble (35 pts UG, 30 PG)\n",
        "\n",
        "In the second question, we will now study the impact of interaction terms (i.e. if you have two variables $x_1$ and $x_2$, adding the multiplication of the terms $x_1 \\cdot x_2$) in XGB models. While XGB is a non-linear model, interactions may help to determine more complex relationships.\n",
        "\n",
        "First, let's reimport the data and create a new dataset.\n",
        "\n",
        "### Question 2.1 Data Import (10% of task's points)\n",
        "\n",
        "a. Reimport the data and separate the target variable (```Closed_Account```) from the input variables (the remaining ones). For the rest of the final, ```credit_score``` will be used as one of the predictors, so be sure to include it.\n",
        "\n",
        "b. Use a ```PolynomialFeatures``` object to create the interactions terms only (i.e. set ```interaction_only=True```). Do not include the biases in the polynomial object either.\n",
        "\n",
        "c. Generate a train/test split using 70% for training and the remaining for testing. Use a random seed of 2022."
      ],
      "id": "I9s5Ud9GoOJz"
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here. You can add as many cells as you want.\n"
      ],
      "metadata": {
        "id": "OBprxMOOsOF9"
      },
      "id": "OBprxMOOsOF9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2.2 (30% of task's points)\n",
        "\n",
        "Now train an XGB model using only the original variables without interaction. *Hint: The variables are ordered so that the first ones are the original ones and the remaining ones the interactions. You can see the variable names by running ```poly_transformer.get_feature_names_out()``` after you fit it, with ```poly_transformer``` the name of your ```PolynomialFeatures``` object.*\n",
        "\n",
        "a. Tune the parameters using the following grid:\n",
        "\n",
        "```\n",
        "param_grid = dict({'n_estimators': [50, 100, 150],\n",
        "                   'max_depth': [2, 3],\n",
        "                 'learning_rate' : [0.001, 0.01, 0.1]\n",
        "                  })\n",
        "```                  \n",
        "Use a ```GridCV``` object with a ```StratifiedKFold``` as input using 3 folds and ```shuffle=False```. Remember to set the ```scale_pos_weight``` Parameter to an appropriate value. Report the best parameters.\n",
        "\n",
        "b. Calculate AUC and plot the ROC curve for the model.\n",
        "\n",
        "c. Plot the SHAP variable importance (beeswarm plot) for the output variables.\n",
        "\n",
        "**Written answer: Analyze the beeswarm plot. What can you see? How do the variables relate to the output? Analyze the distribution of the top three variables.**"
      ],
      "metadata": {
        "id": "pYOI8PlXtE2y"
      },
      "id": "pYOI8PlXtE2y"
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here. You can add as many cells as you want.\n"
      ],
      "metadata": {
        "id": "ywEitaUWyC_E"
      },
      "id": "ywEitaUWyC_E",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Written answer here.**"
      ],
      "metadata": {
        "id": "tll_anes01q1"
      },
      "id": "tll_anes01q1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 2.3 Interactions and conclusion (60% of task's points)\n",
        "\n",
        "Now do the same again, but now use all the training variables, including interactions.\n",
        "\n",
        "a. Tune the parameters using the same ranges as the previous question. **Written answer: How do the parameters change between the previous model and this one? Why do you think that is?**\n",
        "\n",
        "b. Calculate AUC and plot the ROC curve for the model. **Written answer: Compare the prediction capabilities of the two models. What impact do the interactions have on the prediction?**\n",
        "\n",
        "c. Plot the SHAP variable importance (beeswarm plot) for the output variables. **Written answer: Analyze the top three resulting variables and compare them with the previous plot if they were available.**\n",
        "\n",
        "d. **Written answer: Conclude on our original question, comparing the trade-off between interpretability and performance. Do interactions help? Is a non-linear model like XGB able to automatically construct interactions or is it worth it to add them yourself? Why do you think this is?**"
      ],
      "metadata": {
        "id": "Ne8_uvZZuaMQ"
      },
      "id": "Ne8_uvZZuaMQ"
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here. You can add as many cells as you want.\n"
      ],
      "metadata": {
        "id": "rihQ9dr23FDO"
      },
      "id": "rihQ9dr23FDO",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Written answer here.**"
      ],
      "metadata": {
        "id": "mfKhrMpB3SWj"
      },
      "id": "mfKhrMpB3SWj"
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here. You can add as many cells as you want.\n"
      ],
      "metadata": {
        "id": "Mo4zGeZg3Vqs"
      },
      "id": "Mo4zGeZg3Vqs",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Written answer here.**"
      ],
      "metadata": {
        "id": "v9ask2Sv3aId"
      },
      "id": "v9ask2Sv3aId"
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here. You can add as many cells as you want.\n"
      ],
      "metadata": {
        "id": "Dm0sGIur3ZFM"
      },
      "id": "Dm0sGIur3ZFM",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Written answer here.**"
      ],
      "metadata": {
        "id": "nhJVbe-u4peg"
      },
      "id": "nhJVbe-u4peg"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhJwIPReoOJz"
      },
      "source": [
        "## Task 3: Unsupervised learning (30 pts UG, 20 PG)\n",
        "\n",
        "In the previous questions, we could see there seem to be clear groups within our data. In this question we will study whether this is true or not.\n",
        "\n",
        "### Question 3.1 (10% of task's points)\n",
        "\n",
        "Reimport the data once more and normalize them using a MinMaxScaler. Use all variables as input."
      ],
      "id": "WhJwIPReoOJz"
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here. You can add as many cells as you want.\n"
      ],
      "metadata": {
        "id": "cCjhs8A580Rw"
      },
      "id": "cCjhs8A580Rw",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3.2 K-Means (60% of task's points)\n",
        "\n",
        "Run a K-Means algorithm testing between 2 and 8 groups. Use a seed of 2022 for all objects that accept it. Plot both the silhouette plots and the elbow plots.\n",
        "\n",
        "**Written answer: How many clusters do think are in the data?**"
      ],
      "metadata": {
        "id": "iTR46DmF80lo"
      },
      "id": "iTR46DmF80lo"
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here. You can add as many cells as you want.\n"
      ],
      "metadata": {
        "id": "-B1KtFV39N03"
      },
      "id": "-B1KtFV39N03",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Written answer here**"
      ],
      "metadata": {
        "id": "bBfTkvLi0pSR"
      },
      "id": "bBfTkvLi0pSR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Question 3.3 (30% of task's points)\n",
        "\n",
        "Now create a UMAP projection of the data to two dimensions, using 15 nearest neighbours, min_dist of 0.1, spread of 1, an appropriate distance metric, and a random seed of 2022, and plot the resulting scatterplot differentiating the clusters using the cluster membership as colouring. Use the number of clusters you found using the **elbow method**.\n",
        "\n",
        "**Written answer: Justify the metric you used. What can you see in the plot? What can you say about the clusters?**"
      ],
      "metadata": {
        "id": "PpWNh3KK9OSb"
      },
      "id": "PpWNh3KK9OSb"
    },
    {
      "cell_type": "code",
      "source": [
        "# Your code here. You can add as many cells as you want.\n",
        "\n"
      ],
      "metadata": {
        "id": "-h4iUIXx-Aos"
      },
      "id": "-h4iUIXx-Aos",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Written answer here.**"
      ],
      "metadata": {
        "id": "waA95ad26BG4"
      },
      "id": "waA95ad26BG4"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXenWueqoOJ0"
      },
      "source": [
        "## Task 4 (Graduate students only, optional for UG): Neural Networks (25 pts)"
      ],
      "id": "ZXenWueqoOJ0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N81ZR_FOoOJ0"
      },
      "source": [
        "### Question 4.1 Preparing the data. (5 pts)\n",
        "\n",
        "In order to train a neural network, you need to first normalize the data and then create the iterator for the training set. Reimport the data, split it in a training and testing set using ```Closed_Account``` as a target variable and a seed of 2022, and finally standardize the data using a ```StandardScaler``` method."
      ],
      "id": "N81ZR_FOoOJ0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KlDYzlAuoOJ0"
      },
      "outputs": [],
      "source": [
        "# Your code here. You can add as many cells as you want.\n"
      ],
      "id": "KlDYzlAuoOJ0"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oynpzhu6oOJ0"
      },
      "source": [
        "### Question 4.2 (15 pts):\n",
        "Now you are ready to create your architecture. You want to create a dense structure with three hidden layers. Create an architecture with the following properties:\n",
        "\n",
        "- The number of inputs to the network is the number of columns in X_train.\n",
        "- Numbers of neurons for first, second, and third hidden layers are 10, 20, and 6, respectively.\n",
        "- Hidden layers with `relu` activation function.\n",
        "- Apply Dropout regularization with rate of 0.2 after each hidden layer.\n",
        "- Output layer has `sigmoid` activation function with one neuron (as it is a binary problem).\n",
        "- Use `adam` for optimizer, `binary_crossentropy` for loss, and `accuracy` for metrics.\n",
        "- Train the model with 100 epochs with a batch size of 128. Use a validation split of 30%. Balance the sample using the argument class_weight in your fit call (see [here](https://datascience.stackexchange.com/questions/13490/how-to-set-class-weights-for-imbalanced-classes-in-keras) for help)\n",
        "\n",
        "Plot the training history of the model. **Written answer: How many epochs it takes to converge? Do you see overfitting?**"
      ],
      "id": "oynpzhu6oOJ0"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "NZctX2CToOJ1"
      },
      "outputs": [],
      "source": [
        "# Your code here. You can add as many cells as you want.\n"
      ],
      "id": "NZctX2CToOJ1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Written answer here.**"
      ],
      "metadata": {
        "id": "AojnLu-c7oaK"
      },
      "id": "AojnLu-c7oaK"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HLnB1CdroOJ1"
      },
      "source": [
        "### Question 4.3 (5 pts)\n",
        "a. Now it is time to evaluate the model. Find the accuracy for both training and test set.\n",
        "\n",
        "b. Make a confusion matrix for the test set and display it as a heatmap.\n",
        "\n",
        "c. Plot the ROC curve for the test set and display the AUC value. **Written answer: How does it compare to the AUC of the XGB model? Why do you think this is?**"
      ],
      "id": "HLnB1CdroOJ1"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "jupyter": {
          "outputs_hidden": true
        },
        "id": "VqzwB2aSoOJ1"
      },
      "outputs": [],
      "source": [
        "# Your code here. You can add as many cells as you want.\n"
      ],
      "id": "VqzwB2aSoOJ1"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Written answer here.**"
      ],
      "metadata": {
        "id": "4rrRKcQw7BpR"
      },
      "id": "4rrRKcQw7BpR"
    },
    {
      "cell_type": "markdown",
      "source": [
        "**You are done! Remember to re run your notebook to confirm your results and submit within the given time!**"
      ],
      "metadata": {
        "id": "lVVNFElywApQ"
      },
      "id": "lVVNFElywApQ"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Final_2022B.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}