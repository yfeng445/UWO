{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# <span style=\"color:pink\">DS3000B - DS9000B Final Exam</span>\n",
    "\n",
    "## <span style=\"color:pink\">Student ID #: _________</span>\n",
    "\n",
    "## <span style=\"color:pink\">Grade: __ / 100</span>\n",
    "\n",
    "## <span style=\"color:pink\">General Comments</span>\n",
    "\n",
    "- This exam integrates knowledge and skills acquired throughout the term. \n",
    "\n",
    "- Use of chatbots, *e.g.*, ChatGPT, is prohibited.\n",
    "\n",
    "- You are allowed to use any document and source on your computer and the internet, but you are **not** allowed to share documents, post questions to online forums (this includes use of homework helpers such as Chegg), or communicate in any way with people inside or outside the venue. \n",
    "\n",
    "- Having any communication tools (*e.g.*, Discord, Teams, Outlook etc.) either web-based or app-based open on your computer (or having them running in the background) is considered an act of cheating and you will receive 0 mark for the exam.\n",
    "\n",
    "- To finish the exam in the alloted time, you will have to work efficiently. Read the entirety of each question carefully.\n",
    "\n",
    "- You must have your work submitted by 12:00PM today to the \"Test and Quizzes\" section of the course's site on OWL, *i.e.*, the same place where you originally downloaded the notebook. Late submission will be scored with 0 mark. To avoid technical difficulties, start your submission, at the latest, five to ten minutes before the deadline.  \n",
    "\n",
    "- Some questions demand a **written answer** - answer these in a full English sentence in markdown cells. \n",
    "\n",
    "- For your figures, ensure that all axes are labeled in an informative way. To facilitate interpretation, there could be a situation where you should limit the x-axis and/or y-axis to zoom-in for clarity.\n",
    "\n",
    "- At the end, before submitting to OWL, restart the kernel and rerun all cells to make sure that your notebook runs error free and as expected. \n",
    "\n",
    "## <span style=\"color:pink\">Additional Guidance</span>\n",
    "\n",
    "- The \"Toolbox\" cells offer almost every tool that you need to answer the questions, however, depending on your answers, there could be a couple of instances where you'd need to bring in more tools - unless a question imposes some restrictions.\n",
    "- If at any point you are asking yourself \"are we supposed to...\", write your assumptions clearly and proceed according to them.\n",
    "- If you have no clue how to approach a question, skip it, and move on. Revisit the skipped one(s) after you are done with the other questions.\n",
    "- Where applicable, take advantage of the argument `n_jobs=-1` to speed up processes with parallel computing.\n",
    "- To navigate within the notebook, better to take advantage of the notebook's table of contents (normally on the left side of the screen). It saves you time compared to pure mouse scrolling. In VScode, it is nested under the \"OUTLINE\" tab which is by default minimized unless you click it to maximize.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:orange\">Data</span>\n",
    "\n",
    "Connor Andrew McDavid is a Canadian professional ice hockey player and captain of the Edmonton Oilers of the National Hockey League (NHL). The data file `final.csv` provides a reduced version of Connor's game by game career data. Each row represents the stats of one game. The dataset has the following attributes:\n",
    "\n",
    "|#| Attribute | Description |\n",
    "| --- | --- | --- |\n",
    "|0|`opposingTeam`|The team the player played against.|\n",
    "|1|`home_or_away`|Whether a game was played home or away.|\n",
    "|2|`icetime`|Log10 of total time the player played in seconds.|\n",
    "|3|`gameScore`|Game score rating.|\n",
    "|4|`I_F_primaryAssists`|Primary Assists the player has received on teammates' goals.|\n",
    "|5|`I_F_secondaryAssists`|Secondary Assists the player has received on teammates' goals.|\n",
    "|6|`log10_I_F_shotAttempts`|Log10 of shot attempts. Includes player's shots on goal, missed shots, and blocked shot attempts.|\n",
    "|7|`I_F_goals`|Number of goals the player scored.|\n",
    "|8|`I_F_rebounds`|Rebound shot attempts. These must occur within 3 seconds of a previous shot.|\n",
    "|9|`I_F_reboundGoals`|Goals from rebound shot attempts.|\n",
    "|10|`I_F_freeze`|Puck freezes after a player's shot. The  number of puck freezes by goalies after the player's unblocked shot attempts.|\n",
    "|11|`I_F_playContinuedInZone`|Number of times the play continues in the offensive zone after the player's shot besides an immediate rebound shot.|\n",
    "|12|`I_F_playContinuedOutsideZone`|Number of times the play goes outside the offensive zone after the player's shot.|\n",
    "|13|`I_F_savedShotsOnGoal`|Number of the player's unblocked shots that were saved by the goalie.|\n",
    "|14|`I_F_savedUnblockedShotAttempts`|Number of the player's unblocked shots that were saved by the goalie or missed the net.|\n",
    "|15|`I_F_penalityMinutes`|Number of penalty minutes the player has received.|\n",
    "|16|`log10_I_F_faceOffsWon`|Log10 of number of faceoffs the player has won.|\n",
    "|17|`I_F_hits`|Number of hits the player has given.|\n",
    "|18|`I_F_takeaways`|Number of takeaways the player has taken from opponents.|\n",
    "|19|`I_F_giveaways`|Number of giveaways the player has given to other team.|\n",
    "|20|`I_F_lowDangerGoals`|Goals from low danger shots.|\n",
    "|21|`I_F_mediumDangerGoals`|Goals from medium danger shots.|\n",
    "|22|`I_F_highDangerGoals`|Goals from high danger shots.|\n",
    "|23|`I_F_unblockedShotAttempts`|All shot attempts that weren't blocked.|\n",
    "|24|`I_F_dZoneGiveaways`|Giveaways in the team's defensive zone.|\n",
    "|25|`penalityMinutesDrawn`|Number of penalty minutes the player has drawn.|\n",
    "|26|`penaltiesDrawn`|Number of penalties the player has drawn.|"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## <span style=\"color:orange\">Global Toolbox</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "seed = 2023 # work with this seed throughout the notebook\n",
    "np.random.seed(seed)\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from sklearn.cluster import KMeans\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 1 - <span style=\"color:green\">[65]</span> - Supervised Learning\n",
    "\n",
    "We want to find a model which best predicts Connor's icetime, but first we want to do some preprocessing, leading us to dimension-reduced versions of the data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">Q1 Toolbox</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.1 - <span style=\"color:red\">[5]</span> - Apply any data transformation you consider necessary, drop categorical attributes (if any), and create the matrix of predictors and target vector, calling them `X1` and `y`, respectively. What is the `shape` of `X1` and `y`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.2 - <span style=\"color:red\">[10]</span> - Use a 15-component regular PCA to transform `X1` and create the scree plot. Let $p$ be the **minimum** number of PCs required in order to capture at least 80% of total variance. What would be the value of $p$? Reduce the second dimension of `X1` to $p$ and call this new array `X2` (retain `X1` intact though, we need it for later). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.3 - <span style=\"color:red\">[10]</span> - This time, we want to use `KMeans` to transform and reduce the second dimension of `X1`. Transform `X1` into 16 features with the first 15 being the distances of the instances to the k centroids and the 16th being the cluster membership of the instances (*i.e.*, cluster label). Call this new array `X3`, and again keep `X1` intact as we need it for later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.4 - <span style=\"color:red\">[20]</span> - Now that you have 3 different design matrices (*i.e.*, `X1`, `X2`, and `X3`) let's try different scenarios: Train a simple linear regression (with default arguments) once using `X1` (*i.e.*, Model 1), another time by combining `X1` and `X2` (*i.e.*, Model 2), and finally by combining `X1`, `X2`, and `X3` (*i.e.*, Model 3). Use cross-validation with RMSE as the error measure to identify the best model. Report the generalization loss of the winning model and store its design matrix under `X_final`.\n",
    "\n",
    "(For the cross-validation, do five-fold shuffled. For train/test split, use sklearn's default value for test set size.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.5 - <span style=\"color:red\">[5]</span> - Now that you have decided your `X_final`, use it to train an `XGBRegressor` with 150 estimators and remember to use the notebook `seed`. Leave all other parameters at their default values. Report its cross-validated and generalization RMSE. Comparing to your best model in Q1.4, do you achieve a better model or worse?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1.6 - <span style=\"color:red\">[15]</span> - Now, we want to see if we can make the Xgboost regressor any better. Use the cross-validated grid search function to find the best possible values for `max_depth`, and `learning_rate` for the Xgboost regressor. Here are the degrees of freedom to use: for `max_depth` try `[2, 4, 6]`, and for `learning_rate` try `[0.05, 0.1, 0.2]`. Use the best parameters found and report the cross-validated and generalization RMSE of the fine-tuned Xgboost regressor. How does this model compare against all others? Why?\n",
    "\n",
    "Note: To save time, use only 50% of data points (randomly selected from `X_final` and `y`) to fit the grid search function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Question 2 - <span style=\"color:green\">[35]</span> - Unsupervised Learning\n",
    "\n",
    "We want to find out how many quality clusters are there in the data (*i.e.*, `final.csv`), and for this, in addition to the standard clustering metrics, we want to see if UMAP and t-SNE can help with the quest.\n",
    "\n",
    "Note: Apply any data transformation you consider necessary, drop categorical attributes (if any)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"color:orange\">Q2 Toolbox</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score\n",
    "from yellowbrick.cluster import SilhouetteVisualizer\n",
    "import umap\n",
    "import umap.plot\n",
    "from sklearn.manifold import TSNE"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 2.1 - <span style=\"color:red\">[12]</span> - Cluster the data using `KMeans` with `k` in $[2, 3, 4, 5, 6, 10]$. Plot reduction in variance versus `k` and silhouette score versus `k`. Solely relying on these two plots, what are the best three values that you would conclude for `k`? Why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 2.2 - <span style=\"color:red\">[8]</span> - Plot the silhouette diagrams (*i.e.*, the stacked silhouette scores for clusters) for your choices of `k` made in the previous question. Do these help with further narrowing down you choices? Interpret the diagrams to support your answer.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 2.3 - <span style=\"color:red\">[6]</span> - Plot a 2-component UMAP of the data and judge how many clusters do you see in there? Does this help with your quest? Taking this into account, what would be your choice(s) of `k` now?\n",
    "\n",
    "(Use `n_jobs=-1`, `random_state=seed`, and leave all other parameters at their default values.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 2.4 - <span style=\"color:red\">[6]</span> - Plot a 2-component t-SNE (with a perplexity of 100) of the data and judge how many clusters do you see in there? Does this help with your quest?\n",
    "\n",
    "(Use `n_jobs=-1`, `random_state=seed`, and leave the rest at their default values.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q 2.5 - <span style=\"color:red\">[3]</span> - What number of clusters do you ultimately report?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Warning!\n",
    "\n",
    "After uploading your completed notebook to OWL, make sure to click the \"Submit for Grading\" button and confirm your submission. If your submission is successful, you should receive a confirmation email in your UWO inbox.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "my_py_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
